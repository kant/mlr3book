[
["index.html", "mlr3 manual 1 Prerequisites", " mlr3 manual The mlr-org Team 2019-07-12 1 Prerequisites To run all code examples in this book successfully, install the mlr3book package using the remotes package: remotes::install_github(&quot;mlr-org/mlr3book&quot;, dependencies = TRUE) After the installation, load the mlr3 package and you are good to go: library(mlr3) "],
["basics.html", "2 mlr3 Basics ", " 2 mlr3 Basics "],
["introduction.html", "2.1 Introduction", " 2.1 Introduction A typical machine-learning workflow looks like this: We refer to “building blocks” in our guide as steps in the machine-learning workflow. The ones shown above are the essential blocks. Other building blocks can be added to the workflow to enhance certain parts of the essential blocks. The mlr3 package provides R6 class objects for the building blocks tasks, learners and measures. Other building blocks include: Ensemble learners (stacking of models) Feature selection Hyperparameter tuning Parallelization Preprocessing of data Resampling methods These additional blocks are provided by our extension packages such as mlr3tuning, mlr3pipelines or mlr3featuresel. "],
["tasks-learners.html", "2.2 Tasks &amp; Learners", " 2.2 Tasks &amp; Learners Tasks and learners are two essential building blocks that are required to execute any machine-learning experiment. A task wraps the data and store additional information about it. Learners store information about and interface the machine learning algorithms. "],
["tasks.html", "2.3 Tasks", " 2.3 Tasks Tasks are objects that include the data set and additional meta information about a machine-learning problem. This could be the name of the target variable for supervised problems or whether the data set belongs to a specific community of datasets (e.g. a spatial or survival dataset). This information can be used at different points of the workflow to account for specific characteristics of these dataset types. 2.3.1 Task Types To create a task from a data.frame() or data.table() object, the task type needs to be selected: Classification Task: Target column is labels (stored as character()/factor()) with only few distinct values. \\(\\Rightarrow\\) mlr3::TaskClassif Regression Task: Target column is numeric (stored as integer()/double()). \\(\\Rightarrow\\) mlr3::TaskRegr Survival Task: Target is the (right-censored) time to event. \\(\\Rightarrow\\) mlr3survival::TaskSurv in add-on package mlr3surival Ordinal Regression Task: Target is ordinal. \\(\\Rightarrow\\) mlr3ordinal::TaskOrdinal in add-on package mlr3ordinal Cluster Task: You don’t have a target but want to identify similarities in the feature space. \\(\\Rightarrow\\) Not yet implemented Spatial Task: The observations come with spatio-temporal information (e.g. coordinates). \\(\\Rightarrow\\) Not yet implemented in add-on package mlr3spatiotemporal 2.3.2 Task Creation Let’s assume we want to create a simple regression task using the mtcars data set from the package datasets to predict the column \"mpg\" (miles per gallon). For this showcase we only take the first two features to keep the output in the following examples compact. data(&quot;mtcars&quot;, package = &quot;datasets&quot;) data = mtcars[, 1:3] str(data) ## &#39;data.frame&#39;: 32 obs. of 3 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... Next, we create the task by providing the following information: id: Identifier for the task, used in plots and summaries. backend: Here, we simply provide the dataset. It is internally converted to a DataBackendDataTable. For more fine-grain control over how the data is stored internally, we could also construct a DataBackend manually. target: Column name of the target column for the regression problem. library(mlr3) task_mtcars = TaskRegr$new(id = &quot;cars&quot;, backend = data, target = &quot;mpg&quot;) print(task_mtcars) ## &lt;TaskRegr:cars&gt; (32 x 3) ## Target: mpg ## Properties: - ## Features (2): ## * dbl (2): cyl, disp The print() method gives a short summary of the task: It has 32 observations, 3 columns of which 2 columns are features. We can also print the task using the mlr3viz package: library(mlr3viz) autoplot(task_mtcars, type = &quot;pairs&quot;) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 2.3.3 Predefined tasks mlr3 ships with some predefined machine-learning tasks. These are stored in a R6 Dictionary, which is a simple key-value storage named mlr3::mlr_tasks. If we simply print it out, we see that is has 9 entries: mlr_tasks ## &lt;DictionaryTask&gt; with 9 stored values ## Keys: boston_housing, german_credit, iris, mtcars, pima, sonar, spam, ## wine, zoo We can obtain a summarizing overview of all stored tasks by converting the dictionary to a data.table() object library(data.table) as.data.table(mlr_tasks) ## key task_type nrow ncol lgl int dbl chr fct ord pxc ## 1: boston_housing regr 506 19 0 3 13 0 2 0 0 ## 2: german_credit classif 1000 21 0 0 7 0 12 1 0 ## 3: iris classif 150 5 0 0 4 0 0 0 0 ## 4: mtcars regr 32 11 0 0 10 0 0 0 0 ## 5: pima classif 768 9 0 0 8 0 0 0 0 ## 6: sonar classif 208 61 0 0 60 0 0 0 0 ## 7: spam classif 4601 58 0 0 57 0 0 0 0 ## 8: wine classif 178 14 0 2 11 0 0 0 0 ## 9: zoo classif 101 17 15 1 0 0 0 0 0 To create a task from the dictionary (think of it as a book shelf), we use the $get() method from the mlr_tasks class and assign it to a new object. For example, if we would like to use the iris data set for classification: task_iris = mlr_tasks$get(&quot;iris&quot;) print(task_iris) ## &lt;TaskClassif:iris&gt; (150 x 5) ## Target: Species ## Properties: multiclass ## Features (4): ## * dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width 2.3.4 Task API All task properties and characteristics can be queried using the task’s public member values and methods (see Task). task_iris = mlr_tasks$get(&quot;iris&quot;) Member values: Values stored in the object that can be queried by the user # public member values task_iris$nrow ## [1] 150 task_iris$ncol ## [1] 5 Member methods: Functions from the object that can accept arguments and return information stored in the object # public member methods task_iris$head(n = 3) ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## 1: setosa 1.4 0.2 5.1 3.5 ## 2: setosa 1.4 0.2 4.9 3.0 ## 3: setosa 1.3 0.2 4.7 3.2 2.3.4.1 Retrieve Data In mlr3, each row (observation) has a unique identifier which can be either integer or character. These can be used to select specific rows. The iris dataset uses integer row_ids: # iris uses integer row_ids head(task_iris$row_ids) ## [1] 1 2 3 4 5 6 # retrieve data for rows with ids 1, 51, and 101 task_iris$data(rows = c(1, 51, 101)) ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## 1: setosa 1.4 0.2 5.1 3.5 ## 2: versicolor 4.7 1.4 7.0 3.2 ## 3: virginica 6.0 2.5 6.3 3.3 While the mtcars dataset uses names for its row_ids, encoded as character: head(task_mtcars$row_ids) ## [1] &quot;AMC Javelin&quot; &quot;Cadillac Fleetwood&quot; &quot;Camaro Z28&quot; ## [4] &quot;Chrysler Imperial&quot; &quot;Datsun 710&quot; &quot;Dodge Challenger&quot; # retrieve data for rows with id &quot;Datsun 710&quot; task_mtcars$data(rows = &quot;Datsun 710&quot;) ## mpg cyl disp ## 1: 22.8 4 108 Note that the method $data() is only an accessor and does not modify the underlying data/task. Analogously, each column has an identifier, which is often just called “column name”. These are stored in the public slots feature_names and target_names. Here “target” refers to the response variable and “feature” to the predictor variables of the task. task_iris$feature_names ## [1] &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; task_iris$target_names ## [1] &quot;Species&quot; The row_id names and the “column names” can be combined for subsetting: # retrieve data for rows 1, 51, and 101 and only select column &quot;Species&quot; task_iris$data(rows = c(1, 51, 101), cols = &quot;Species&quot;) ## Species ## 1: setosa ## 2: versicolor ## 3: virginica To extract the complete dataset from the task, we can simply convert the task to a data.table: summary(as.data.table(task_iris)) ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## setosa :50 Min. :1.00 Min. :0.1 Min. :4.30 Min. :2.00 ## versicolor:50 1st Qu.:1.60 1st Qu.:0.3 1st Qu.:5.10 1st Qu.:2.80 ## virginica :50 Median :4.35 Median :1.3 Median :5.80 Median :3.00 ## Mean :3.76 Mean :1.2 Mean :5.84 Mean :3.06 ## 3rd Qu.:5.10 3rd Qu.:1.8 3rd Qu.:6.40 3rd Qu.:3.30 ## Max. :6.90 Max. :2.5 Max. :7.90 Max. :4.40 2.3.4.2 Roles (Rows and Columns) It is possible to assign a special meanings (aka “roles”) to (subsets of) rows and columns. For example, the previously constructed mtcars task has the following column roles: print(task_mtcars$col_roles) ## $feature ## [1] &quot;cyl&quot; &quot;disp&quot; ## ## $target ## [1] &quot;mpg&quot; ## ## $label ## character(0) ## ## $order ## character(0) ## ## $groups ## character(0) ## ## $weights ## character(0) Now, we want the original rownames() of mtcars to be a regular feature column. Thus, we first preprocess the data.frame and then re-create the task. # with `keep.rownames`, data.table stores the row names in an extra column &quot;rn&quot; data = as.data.table(mtcars[, 1:3], keep.rownames = TRUE) task = TaskRegr$new(id = &quot;cars&quot;, backend = data, target = &quot;mpg&quot;) # we now have integer row_ids task$row_ids ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 31 32 # there is a new &quot;feature&quot; called &quot;rn&quot; task$feature_names ## [1] &quot;cyl&quot; &quot;disp&quot; &quot;rn&quot; The column “rn” is now a regular feature. As this is a unique string column, most machine-learning algorithms will have problems to process this feature without some kind of preprocessing. However, we still might want to carry rn around for different reasons. For example, we can use the row names in plots or to associate outliers with the row names. This being said, we need to change the role of the row names column rn and remove it from the set of active features. task$feature_names ## [1] &quot;cyl&quot; &quot;disp&quot; &quot;rn&quot; task$set_col_role(&quot;rn&quot;, new_roles = &quot;label&quot;) # &quot;rn&quot; not listed as feature any more task$feature_names ## [1] &quot;cyl&quot; &quot;disp&quot; # also vanished from &quot;data&quot; and &quot;head&quot; task$data(rows = 1:2) ## mpg cyl disp ## 1: 21 6 160 ## 2: 21 6 160 task$head(2) ## mpg cyl disp ## 1: 21 6 160 ## 2: 21 6 160 Note that no copies of the underlying data is inflicted by this operation. By changing roles, only the view on the data is changed, not the data itself. Just like columns, it is also possible to assign different roles to rows. Rows can have two different roles: Role \"use\": Rows that are generally available for model fitting (although they may also be used as test set in resampling). This is the default role. Role \"validation\": Rows that are held back (see below). Rows which have missing values in the target column upon task creation are automatically moved to the validation set. There are several reasons to hold some observations back or treat them differently: It is often good practice to validate the final model on an external validation set to uncover possible overfitting. Some observations may be unlabeled, e.g. in data mining cups or Kaggle competitions. These observations cannot be used for training a model, but you can still predict labels. 2.3.4.3 Task Mutators Task methods .$set_col_role() and .$set_row_role() change the view on the data and can be used to subset the task. For convenience, method .$filter() subsets the task based on row ids, and .$select() subsets the task based on feature names. All these operations only change the view on the data, without creating a copy of it, but modify the task in-place. task = mlr_tasks$get(&quot;iris&quot;) task$select(c(&quot;Sepal.Width&quot;, &quot;Sepal.Length&quot;)) # keep only these features task$filter(1:3) # keep only these rows task$head() ## Species Sepal.Length Sepal.Width ## 1: setosa 5.1 3.5 ## 2: setosa 4.9 3.0 ## 3: setosa 4.7 3.2 Additionally, methods .$rbind() and .$cbind() allow to add extra rows and columns to a task, respectively. Again, the original data set stored in the original DataBackend is not altered in any way. task$cbind(data.table(foo = letters[1:3])) # add column foo task$head() ## Species Sepal.Length Sepal.Width foo ## 1: setosa 5.1 3.5 a ## 2: setosa 4.9 3.0 b ## 3: setosa 4.7 3.2 c "],
["learners.html", "2.4 Learners", " 2.4 Learners Objects of class mlr3::Learner provide a unified interface to many popular machine-learning algorithms in R. They consist of methods to train and predict on a mlr3::Task, and additionally provide meta information about the algorithms. The package ships with only a rather minimal set of classification and regression learners, more are implemented in the mlr3learners package. Furthermore, mlr3learners has some documentation on creating custom learners. 2.4.1 Predefined Learners Similar to mlr3::mlr_tasks, the mlr3misc::Dictionary mlr3::mlr_learners can be queried for available learners: mlr_learners ## &lt;DictionaryLearner&gt; with 5 stored values ## Keys: classif.debug, classif.featureless, classif.rpart, ## regr.featureless, regr.rpart as.data.table(mlr_learners) ## key feature_types ## 1: classif.debug logical,integer,numeric,character,factor,ordered ## 2: classif.featureless logical,integer,numeric,character,factor,ordered ## 3: classif.rpart logical,integer,numeric,character,factor,ordered ## 4: regr.featureless logical,integer,numeric,character,factor,ordered ## 5: regr.rpart logical,integer,numeric,character,factor,ordered ## packages properties ## 1: missings,multiclass,twoclass ## 2: importance,missings,multiclass,selected_features,twoclass ## 3: rpart importance,missings,multiclass,selected_features,twoclass,weights ## 4: stats importance,missings,selected_features ## 5: rpart importance,missings,selected_features,weights ## predict_types ## 1: response,prob ## 2: response,prob ## 3: response,prob ## 4: response,se ## 5: response As listed in the output, each learner comes with the following information: feature_types: what kind of features can be processed. packages: which packages are required to run train() and predict(). properties: additional properties and capabilities. For example, a learner has the property “missings” if it is able to handle missing values, and “importance” if it is possible to extract feature importance values. predict_types: what predict types are possible. For example, a classification learner can predict labels (“response”) or probabilities (“prob”). To extract a specific learner, use the corresponding \"id\": learner = mlr_learners$get(&quot;classif.rpart&quot;) print(learner) ## &lt;LearnerClassifRpart:classif.rpart&gt; ## Model: - ## Parameters: xval=0 ## Packages: rpart ## Predict Type: response ## Feature types: logical, integer, numeric, character, factor, ordered ## Properties: importance, missings, multiclass, selected_features, ## twoclass, weights In the output we see that all information from the previous table is also accessible via public slots (id, feature_types, packages, properties, predict_types). Additionally, predict_type returns the currently selected predict type of the learner. Slot param_set stores a description of hyperparameter settings: learner$param_set ## ParamSet: ## id class lower upper levels default value ## 1: minsplit ParamInt 1 Inf 20 ## 2: cp ParamDbl 0 1 0.01 ## 3: maxcompete ParamInt 0 Inf 4 ## 4: maxsurrogate ParamInt 0 Inf 5 ## 5: maxdepth ParamInt 1 30 30 ## 6: xval ParamInt 0 Inf 10 0 The set of hyperparameter values is stored inside the parameter set in the values slot. By assigning a named list to this slot, we change the active hyperparameters of the learner: learner$param_set$values = list(cp = 0.01) learner ## &lt;LearnerClassifRpart:classif.rpart&gt; ## Model: - ## Parameters: cp=0.01 ## Packages: rpart ## Predict Type: response ## Feature types: logical, integer, numeric, character, factor, ordered ## Properties: importance, missings, multiclass, selected_features, ## twoclass, weights The slot model stores the result of the training step. As we have not yet trained a model, this slot is NULL: learner$model ## NULL "],
["train-predict.html", "2.5 Train &amp; Predict", " 2.5 Train &amp; Predict In this chapter, we explain how tasks and learners can be used to train a model and predict to a new dataset. The concept is demonstrated on a supervised classification using the iris dataset and the rpart learner (classification tree). Additionally, this chapter includes the following use-cases Functional Data Analysis using (WIP) Regression Analysis using (WIP) Survival Analysis using (WIP) Spatial Analysis using (WIP) 2.5.1 Basic concept 2.5.1.1 Creating Task and Learner Objects The first step is to generate the following mlr3 objects from the task dictionary and the learner dictionary, respectively: The classification task task = mlr_tasks$get(&quot;iris&quot;) A learner for the classification tree learner = mlr_learners$get(&quot;classif.rpart&quot;) 2.5.1.2 Setting up the train/test splits of the data (#split-data) It is common to train on a majority of the data. Here we use 80% of all available observations and predict on the remaining 20% observations. For this purpose, we create two index vectors: train_set = sample(task$nrow, 0.8 * task$nrow) test_set = setdiff(seq_len(task$nrow), train_set) 2.5.1.3 Training the learner Next, we train the classification tree on the train set of the iris task using the $train() method of the Learner: learner$train(task, row_ids = train_set) This operation modifies the learner in-place. We can access the stored model via the field $model: print(learner$model) ## n= 120 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 120 79 versicolor (0.33333 0.34167 0.32500) ## 2) Petal.Length&lt; 2.45 40 0 setosa (1.00000 0.00000 0.00000) * ## 3) Petal.Length&gt;=2.45 80 39 versicolor (0.00000 0.51250 0.48750) ## 6) Petal.Width&lt; 1.65 43 3 versicolor (0.00000 0.93023 0.06977) * ## 7) Petal.Width&gt;=1.65 37 1 virginica (0.00000 0.02703 0.97297) * 2.5.1.4 Predicting After the model was trained, we use the remaining part of the data for prediction. Remember that we initially split the data in train_set and test_set. prediction = learner$predict(task, row_ids = test_set) print(prediction) ## &lt;PredictionClassif&gt; for 30 observations: ## row_id truth response ## 1: 1 setosa setosa ## 2: 2 setosa setosa ## 3: 3 setosa setosa ## --- ## 28: 138 virginica virginica ## 29: 142 virginica virginica ## 30: 146 virginica virginica The $predict() method of the Learner returns a Prediction object. More precise, as the learner is specialized for classification, a LearnerClassif returns a PredictionClassif object. A prediction objects holds The row ids of the test data, the respective true label of the target column and the respective predictions. The simplest way to extract this information is by converting to a data.table(): head(as.data.table(prediction)) ## row_id truth response ## 1: 1 setosa setosa ## 2: 2 setosa setosa ## 3: 3 setosa setosa ## 4: 6 setosa setosa ## 5: 13 setosa setosa ## 6: 18 setosa setosa For classification, you can also extract the confusion matrix: prediction$confusion ## truth ## response setosa versicolor virginica ## setosa 10 0 0 ## versicolor 0 8 1 ## virginica 0 1 10 2.5.1.4.1 Performance assessment The last step of an modeling is usually the performance assessment where we choose performance measure to quantify the predictions by comparing the predicted labels with the true labels. Available measures are stored in mlr_measures: mlr_measures ## &lt;DictionaryMeasure&gt; with 28 stored values ## Keys: classif.acc, classif.auc, classif.ce, classif.costs, classif.f1, ## classif.fdr, classif.fn, classif.fnr, classif.for, classif.fp, ## classif.fpr, classif.npv, classif.ppv, classif.precision, ## classif.recall, classif.sensitivity, classif.specificity, classif.tn, ## classif.tnr, classif.tp, classif.tpr, oob_error, regr.mae, regr.mse, ## selected_features, time_both, time_predict, time_train We select the accuracy (classif.acc) and call the method $score() of the Prediction object: measure = mlr_measures$get(&quot;classif.ce&quot;) prediction$score(measure) ## classif.ce ## 0.06667 Note that, if no measure is specified, classification defaults to classification error (classif.ce) and regression defaults to the mean squared error (regr.mse). "],
["resampling.html", "2.6 Resampling", " 2.6 Resampling 2.6.1 Settings In this example we use the iris task and a simple classification tree (package rpart). task = mlr_tasks$get(&quot;iris&quot;) learner = mlr_learners$get(&quot;classif.rpart&quot;) When performing resampling with a dataset, we first need to define which approach should be used. The resampling strategies of mlr3 can be queried using the .$keys() method of the mlr_resamplings dictionary. mlr_resamplings ## &lt;DictionaryResampling&gt; with 7 stored values ## Keys: bootstrap, custom, cv, cv3, holdout, repeated_cv, subsampling Additional resampling methods for special use cases will be available via extension packages, such as mlr3survival for survival analysis or mlr3spatiotemporal for spatial data (still in development). The model fit conducted in the train/predict/score chapter is equivalent to a “holdout”, so let’s consider this one first. resampling = mlr_resamplings$get(&quot;holdout&quot;) print(resampling) ## &lt;ResamplingHoldout&gt; with 1 iterations ## Instantiated: FALSE ## Parameters: ratio=0.6667 Note that the Instantianated field is set to FALSE. This means we did not actually apply the strategy on a dataset yet but just performed a dry-run. Applying the strategy on a dataset is done in section next Instantation. By default we get a .66/.33 split of the data. There are two ways how the ratio can be changed: Overwriting the slot in .$param_set$values using a named list. resampling$param_set$values = list(ratio = 0.8) Specifying the resampling parameters directly during construction using the param_vals argument: mlr_resamplings$get(&quot;holdout&quot;, param_vals = list(ratio = 0.8)) ## &lt;ResamplingHoldout&gt; with 1 iterations ## Instantiated: FALSE ## Parameters: ratio=0.8 2.6.2 Instantiation So far we just set the stage and selected the resampling strategy. To actually perform the splitting, we need to apply the settings on a dataset. This can be done in two ways: Manually by calling the method .$instantiate() on a Task resampling = mlr_resamplings$get(&quot;cv&quot;, param_vals = list(folds = 3L)) resampling$instantiate(task) resampling$iters ## [1] 3 resampling$train_set(1) ## [1] 1 2 4 5 7 12 14 15 17 18 21 22 23 24 25 28 30 35 ## [19] 40 42 43 48 49 53 54 58 59 60 67 69 70 73 78 88 92 93 ## [37] 103 104 108 110 114 115 137 138 139 141 142 143 144 145 3 9 10 11 ## [55] 13 16 26 27 31 32 38 44 46 51 56 57 61 68 72 74 76 77 ## [73] 81 86 89 94 95 96 98 101 102 106 109 111 112 113 120 121 123 125 ## [91] 126 129 131 133 134 135 146 147 148 149 Automatically by passing the resampling object to resample(). Here, the splitting is done within the resample() call based on the supplied Task. learner1 = mlr_learners$get(&quot;classif.rpart&quot;) # simple classification tree learner2 = mlr_learners$get(&quot;classif.featureless&quot;) # featureless learner, prediction majority class rr1 = resample(task, learner1, resampling) rr2 = resample(task, learner2, resampling) setequal(rr1$resampling$train_set(1), rr2$resampling$train_set(1)) ## [1] TRUE If you want to compare multiple learners, you should use the same resampling per task to reduce the variance of the performance estimation (method 1). If you use method 2 (and do not instantiate manually before), the resampling splits will differ between both runs. If you aim is to compare different Task, Learner or Resampling, you are better off using the benchmark() function. It is basically a wrapper around resample() simplifying the handling of multiple settings. If you discover this only after you’ve run multiple resample() calls, don’t worry - you can transform multiple single ResampleResult objects into a BenchmarkResult (explained later) using the .$combine() method. 2.6.3 Execution With a Task, a Learner and Resampling object we can call resample() and create a ResampleResult object. rr = resample(task, learner, resampling) print(rr) ## &lt;ResampleResult&gt; of 3 iterations ## Task: iris ## Learner: classif.rpart Before we go into more detail, let’s change the resampling to a “3-fold cross-validation” to better illustrate what operations are possible with a ResampleResult. Additionally, we tell resample() to keep the fitted models via a control object (see mlr_control()): resampling = mlr_resamplings$get(&quot;cv&quot;, param_vals = list(folds = 3L)) ctrl = mlr_control(store_models = TRUE) rr = resample(task, learner, resampling, ctrl = ctrl) print(rr) ## &lt;ResampleResult&gt; of 3 iterations ## Task: iris ## Learner: classif.rpart The following operations are supported with ResampleResult objects: Extract the performance for the individual resampling iterations: rr$performance(&quot;classif.ce&quot;) ## task task_id learner learner_id resampling ## 1: &lt;TaskClassif&gt; iris &lt;LearnerClassifRpart&gt; classif.rpart &lt;ResamplingCV&gt; ## 2: &lt;TaskClassif&gt; iris &lt;LearnerClassifRpart&gt; classif.rpart &lt;ResamplingCV&gt; ## 3: &lt;TaskClassif&gt; iris &lt;LearnerClassifRpart&gt; classif.rpart &lt;ResamplingCV&gt; ## resampling_id iteration prediction classif.ce ## 1: cv 1 &lt;PredictionClassif&gt; 0.04 ## 2: cv 2 &lt;PredictionClassif&gt; 0.06 ## 3: cv 3 &lt;PredictionClassif&gt; 0.10 Extract and inspect the resampling splits: rr$resampling ## &lt;ResamplingCV&gt; with 3 iterations ## Instantiated: TRUE ## Parameters: folds=3 rr$resampling$iters ## [1] 3 rr$resampling$test_set(1) ## [1] 5 8 11 13 15 20 23 26 28 30 31 33 37 39 40 42 45 49 52 ## [20] 54 56 60 63 67 73 74 76 80 81 83 86 88 91 92 97 99 103 105 ## [39] 109 111 117 122 127 131 133 145 147 148 149 150 rr$resampling$train_set(3) ## [1] 5 8 11 13 15 20 23 26 28 30 31 33 37 39 40 42 45 49 ## [19] 52 54 56 60 63 67 73 74 76 80 81 83 86 88 91 92 97 99 ## [37] 103 105 109 111 117 122 127 131 133 145 147 148 149 150 1 3 6 7 ## [55] 14 17 21 27 34 35 43 47 48 50 51 55 59 62 64 70 71 72 ## [73] 75 78 82 85 87 101 104 106 107 108 110 112 113 115 116 121 125 126 ## [91] 128 129 130 132 136 137 139 140 143 144 Retrieve the learner of a specific iteration and inspect it: lrn = rr$learners[[1]] lrn$model ## n= 100 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 100 64 virginica (0.32000 0.32000 0.36000) ## 2) Petal.Length&lt; 2.6 32 0 setosa (1.00000 0.00000 0.00000) * ## 3) Petal.Length&gt;=2.6 68 32 virginica (0.00000 0.47059 0.52941) ## 6) Petal.Length&lt; 4.85 31 2 versicolor (0.00000 0.93548 0.06452) * ## 7) Petal.Length&gt;=4.85 37 3 virginica (0.00000 0.08108 0.91892) * 2.6.4 Custom resampling Sometimes it is necessary to perform resampling with custom splits. If you want to do that because you are coming from a specific modeling field, take a look first at the mlr3 extension packages to make sure your custom resampling method hasn’t been implemented already. If your custom resampling method is widely used in your field, feel welcome to integrate it into one of the existing mlr3 extension packages or create your own one. A manual resampling instance can be created using the \"custom\" template from the mlr_resamplings dictionary. resampling = mlr_resamplings$get(&quot;custom&quot;) resampling$instantiate(task, list(c(1:10, 51:60, 101:110)), list(c(11:20, 61:70, 111:120)) ) resampling$iters ## [1] 1 resampling$train_set(1) ## [1] 1 2 3 4 5 6 7 8 9 10 51 52 53 54 55 56 57 58 59 ## [20] 60 101 102 103 104 105 106 107 108 109 110 resampling$test_set(1) ## [1] 11 12 13 14 15 16 17 18 19 20 61 62 63 64 65 66 67 68 69 ## [20] 70 111 112 113 114 115 116 117 118 119 120 "],
["benchmarking.html", "2.7 Benchmarking", " 2.7 Benchmarking Comparing the performance of different learners on multiple tasks and/or different resampling schemes is a recurrent task. This operation is usually referred to as “benchmarking” in the field of machine-learning. mlr3 offers the benchmark() function for convenience. 2.7.1 Design Creation In mlr3 we require you to supply a “design” of your benchmark experiment. By “design” we essentially mean the matrix of settings you want to execute. A “design” consists of Task, Learner and Resampling. Additionally, you can supply different Measure along side. Here, we call benchmark() to perform a single holdout split on a single task and two learners. We use the expand_grid() function to create an exhaustive design and properly instantiate the resampling: library(data.table) design = expand_grid( tasks = mlr_tasks$mget(&quot;iris&quot;), learners = mlr_learners$mget(c(&quot;classif.rpart&quot;, &quot;classif.featureless&quot;)), resamplings = mlr_resamplings$mget(&quot;holdout&quot;) ) print(design) ## task learner resampling ## 1: &lt;TaskClassif&gt; &lt;LearnerClassifRpart&gt; &lt;ResamplingHoldout&gt; ## 2: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingHoldout&gt; bmr = benchmark(design) Note that the holdout splits have been automatically instantiated for each row of the design. As a result, the rpart learner used a different training set than the featureless learner. However, for comparison of learners you usually want the learners to see the same splits into train and test sets. To overcome this issue, the resampling strategy needs to be manually instantiated before creating the design. While the interface of benchmark() allows full flexibility, the creation of such design tables can be tedious. Therefore, mlr3 provides a convenience function to quickly generate design tables and instantiate resampling strategies in an exhaustive grid fashion: expand_grid(). # get some example tasks tasks = mlr_tasks$mget(c(&quot;pima&quot;, &quot;sonar&quot;, &quot;spam&quot;)) # get some measures: accuracy (acc) and area under the curve (auc) measures = mlr_measures$mget(c(&quot;classif.acc&quot;, &quot;classif.auc&quot;)) # get a featureless learner and a classification tree # let both learners predict probabilities learners = mlr_learners$mget(c(&quot;classif.featureless&quot;, &quot;classif.rpart&quot;), predict_type = &quot;prob&quot;) # compare via 3-fold cross validation resamplings = mlr_resamplings$mget(&quot;cv3&quot;) # create a BenchmarkDesign object design = expand_grid(tasks, learners, resamplings) print(design) ## task learner resampling ## 1: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingCV&gt; ## 2: &lt;TaskClassif&gt; &lt;LearnerClassifRpart&gt; &lt;ResamplingCV&gt; ## 3: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingCV&gt; ## 4: &lt;TaskClassif&gt; &lt;LearnerClassifRpart&gt; &lt;ResamplingCV&gt; ## 5: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingCV&gt; ## 6: &lt;TaskClassif&gt; &lt;LearnerClassifRpart&gt; &lt;ResamplingCV&gt; 2.7.2 Execution and Aggregation of Results After the benchmark design is ready, we can directly call benchmark() # execute the benchmark bmr = benchmark(design) Note that we did not instantiate the resampling instance, but expand_grid() took care of it for us: each resampling strategy is instantiated for each task during the construction of the exhaustive grid. After the benchmark, we can calculate and aggregate the performance with .$aggregate(): bmr$aggregate(measures) ## hash resample_result task_id learner_id resampling_id ## 1: cf5449b9ea5c7a66 &lt;ResampleResult&gt; pima classif.featureless cv3 ## 2: ac4bfffc9398c6ff &lt;ResampleResult&gt; pima classif.rpart cv3 ## 3: 242bb105654050fe &lt;ResampleResult&gt; sonar classif.featureless cv3 ## 4: ca100d5fc1a6af96 &lt;ResampleResult&gt; sonar classif.rpart cv3 ## 5: 75d6e6947ab1ad07 &lt;ResampleResult&gt; spam classif.featureless cv3 ## 6: ffddcb535456bccd &lt;ResampleResult&gt; spam classif.rpart cv3 ## classif.acc classif.auc ## 1: 0.6510 0.5000 ## 2: 0.7357 0.7715 ## 3: 0.5337 0.5000 ## 4: 0.6972 0.7839 ## 5: 0.6060 0.5000 ## 6: 0.9002 0.9027 We can aggregate the results further. For example, we might be interested which learner performed best over all tasks. Since we have data.table object here, we could do the following: bmr$aggregate(measures)[, list(acc = mean(classif.acc), auc = mean(classif.auc)), by = &quot;learner_id&quot;] ## learner_id acc auc ## 1: classif.featureless 0.5969 0.5000 ## 2: classif.rpart 0.7777 0.8193 Alternatively, we can also use the tidyverse approach: library(&quot;magrittr&quot;) requireNamespace(&quot;dplyr&quot;) requireNamespace(&quot;tibble&quot;) bmr$aggregate(measures) %&gt;% tibble::as_tibble() %&gt;% dplyr::group_by(learner_id) %&gt;% dplyr::summarise(acc = mean(classif.acc), auc = mean(classif.auc)) ## # A tibble: 2 x 3 ## learner_id acc auc ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 classif.featureless 0.597 0.5 ## 2 classif.rpart 0.778 0.819 Unsurprisingly, the classification tree outperformed the featureless learner. 2.7.3 Converting specific benchmark objects to resample objects A BenchmarkResult object is essentially a collection of multiple ResampleResult objects. As these are stored in a column of the aggregated data.table(), we can easily extract them: tab = bmr$aggregate(measures) rr = tab[task_id == &quot;spam&quot; &amp; learner_id == &quot;classif.rpart&quot;]$resample_result[[1]] print(rr) ## &lt;ResampleResult&gt; of 3 iterations ## Task: spam ## Learner: classif.rpart We can now investigate this resampling and even single resampling iterations using one of the approach shown in the previous section: rr$aggregate(measure) ## classif.ce ## 0.09976 # get the iteration with worst AUC worst = rr$performance(measures) %&gt;% tibble::as_tibble() %&gt;% dplyr::slice(which.min(classif.auc)) %&gt;% dplyr::select(classif.auc, iteration) # get the corresponding learner lrn = rr$learners[[worst$iteration]] lrn ## &lt;LearnerClassifRpart:classif.rpart&gt; ## Model: - ## Parameters: xval=0 ## Packages: rpart ## Predict Type: prob ## Feature types: logical, integer, numeric, character, factor, ordered ## Properties: importance, missings, multiclass, selected_features, ## twoclass, weights "],
["binary.html", "2.8 Binary classification", " 2.8 Binary classification 2.8.1 ROC 2.8.2 Thresholds "],
["mlr-mlr3-transition-guide.html", "2.9 mlr -&gt; mlr3 Transition Guide", " 2.9 mlr -&gt; mlr3 Transition Guide In case you have already worked with mlr, you may want quickstart with mlr3 by looking up the specific equivalent of an element of mlr in the new version mlr3. For this, you can use the following table. This table is not complete but should give you an overview about how mlr3 is organized. Category mlr mlr3 Note General / Helper getCacheDir() / deleteCacheDir() Not yet implemented — getCacheDir() / deleteCacheDir() Not yet implemented — configureMlr() — — getMlrOptions() — mlr_control() createDummyFeatures() Not yet implemented mlr3pipelines crossover() — — downsample() Not yet implemented — generateCalibrationData() Not yet implemented — generateCritDifferencesData() Not yet implemented — generateLearningCurveData() Not yet implemented mlr3viz generatePartialDependenceData() Not yet implemented mlr3viz generateThreshVsPerfData() Not yet implemented mlr3viz getCaretParamSet() Not used anymore — impute() Not yet implemented mlr3pipelines Task Task mlr_tasks / Task — SurvTask TaskSurv mlr3survival ClusterTask mlr_tasks — MultilabelTask mlr_tasks — SpatialTask Not yet implemented mlr3spatiotemporal Example tasks (iris.task, mtcars.task) mlr_tasks$get(‘iris’) — convertMLBenchObjToTask() Not yet implemented mlr3 dropFeatures() Task$select() — getTaskCosts() Not yet implemented — getTaskData() Task$data() — getTaskDesc() / getTaskDescription() Task$print() — getTaskFeatureNames() Task$feature_names — getTaskFormula() Task$formula — getTaskId() Task$id — getTaskNFeats() length(Task$feature_names) — getTaskSize() Task$nrow() — getTaskTargetNames() Task$target_names — getTaskTargets() Task$truth() — getTaskType() Task$task_type — — Learner helpLearner() Not yet implemented — helpLearnerParam() Not yet implemented — getLearnerId() Learner$id — getLearnerModel() Learner$model — getLearnerNote() Not used anymore — getLearnerPackages() Learner$packages — getLearnerParVals() / getLearnerParamSet() Learner$param_set — getLearnerPredictType() Learner$predict_type — getLearnerShortName() Learner$predict_type — getLearnerType() Learner$Type — getParamSet() Learner$param_set — generateLearningCurveData() Not yet implemented mlr3viz FailureModel — — getFailureModelDump() — — getFailureModelMsg() — — isFailureModel() — — Train/Predict/Resample train() Learner$train() — predict() Learner$predict() — performance() Prediction$score() — makeResampleDesc() Resampling, mlr_resamplings — resample() resample() — Aggregation / makeAggregation Not yet implemented — asROCRPrediction() Not yet implemented — getConfMatrix() / calculateConfusionMatrix() Prediction$confusion — calculateROCMeasures() Not yet implemented — estimateRelativeOverfitting() Not yet implemented — estimateResidualVariance() Not yet implemented — getDefaultMeasure() — Benchmark benchmark() benchmark() — batchmark() / reduceBatchmarkResults() not used anymore — BenchmarkResult BenchmarkResult — convertBMRToRankMatrix() Not yet implemented — convertMLBenchObjToTask() Not yet implemented — getBMRAggrPerformances() BenchmarkResult$aggregate() — getBMRFeatSelResults() Not yet implemented mlr3featsel getBMRFilteredFeatures() Not yet implemented mlr3featsel getBMRLearners() / getBMRLearnerIds() / getBMRLearnerShortNames() BenchmarkResult$learners — getBMRMeasures() / getBMRMeasureIds() BenchmarkResult$measures — getBMRModels() BenchmarkResult\\(data\\)learner[[1]]$model — getBMRPerformances() BenchmarkResult\\(data\\)performance — getBMRTaskDescriptions() / getBMRTaskDescs() / getBMRTaskIds() BenchmarkResult$tasks — getBMRTuneResults() Not yet implemented — friedmanTestBMR() Not yet implemented — mergeBenchmarkResults() BenchmarkResult$combine() — plotBMRBoxplots() Not yet implemented mlr3viz plotBMRRanksAsBarChart() Not yet implemented mlr3viz plotBMRSummary() Not yet implemented mlr3viz plotResiduals() Not yet implemented mlr3viz Parameter Specification ParamHelpers::makeNumericParam() ParamDbl$new() paradox ParamHelpers::makeNumericVectorParam() ParamDbl$new() paradox ParamHelpers::makeIntegerParam() paradox::ParamInt$new() paradox ParamHelpers::makeIntegerVectorParam() paradox::ParamInt$new() paradox ParamHelpers::makeDiscreteParam() paradox::ParamFct$new() paradox ParamHelpers::makeDiscreteVectorParam() paradox::ParamFct$new() paradox ParamHelpers::makeLogicalParam() paradox::ParamLgl$new() paradox ParamHelpers::makeLogicalVectorParam() paradox::ParamLgl$new() paradox Preprocessing — — — — — — Feature Selection makeFeatSelControlExhaustive() Not yet implemented mlr3featsel makeFeatSelControlRandom() Not yet implemented mlr3featsel makeFeatSelControlSequential() Not yet implemented mlr3featsel makeFeatSelControlGA() Not yet implemented mlr3featsel makeFilter() Filter$new() mlr3featsel FeatSelResult Not yet implemented mlr3featsel listFilterMethods() mlr_filters mlr3featsel analyzeFeatSelResult() Not yet implemented mlr3featsel getBMRFeatSelResults() Not yet implemented mlr3featsel getBMRFilteredFeatures() Not yet implemented mlr3featsel getFeatSelResult() Not yet implemented mlr3featsel getFeatureImportance() Not yet implemented mlr3featsel getFilteredFeatures() Not yet implemented mlr3featsel makeFeatSelWrapper() Not used anymore mlr3featsel makeFilterWrapper() Not used anymore mlr3featsel getResamplingIndices() Not yet implemented selectFeatures() Not yet implemented mlr3featsel filterFeatures() Filter$filter_*() mlr3featsel generateFilterValuesData() Filter$calculate() mlr3featsel Tuning getTuneResult() AutoTuner\\(learner[[&amp;lt;id&amp;gt;]]\\)tune_path mlr3tuning getTuneResultOptPath() Not yet implemented mlr3tuning Parallelization ParallelMap::parallelStart*(), parallelMap::parallelStop() future::plan() / future Plotting plotBMRBoxplots() Not yet implemented mlr3viz plotBMRRanksAsBarChart() Not yet implemented mlr3viz plotBMRSummary() Not yet implemented mlr3viz plotCalibration() Not yet implemented mlr3viz plotCritDifferences() Not yet implemented mlr3viz plotFilterValues() Not yet implemented mlr3viz plotHyperParsEffect() Not yet implemented mlr3viz plotLearnerPrediction() Not yet implemented mlr3viz plotLearningCurve() Not yet implemented mlr3viz plotPartialDependence() Not yet implemented mlr3viz plotResiduals() Not yet implemented mlr3viz plotROCCurves() Not yet implemented mlr3viz plotThreshVsPerf() Not yet implemented mlr3viz plotTuneMultiCritResult() Not yet implemented mlr3viz FDA extractFDAFPCA() Not yet implemented mlr3fda extractFDAFourier() Not yet implemented mlr3fda extractFDAMultiResFeatures() Not yet implemented mlr3fda extractFDAWavelets() Not yet implemented mlr3fda "],
["model-optim.html", "3 Model Optimization ", " 3 Model Optimization "],
["tuning.html", "3.1 Hyperparameter Tuning", " 3.1 Hyperparameter Tuning Hyperparameter tuning is supported via the extension package mlr3tuning. The heart of mlr3tuning are the R6 classes mlr3tuning::PerformanceEvaluator and the Tuner* classes. They store the settings, perform the tuning and save the results. 3.1.1 The Performance Evaluator class The mlr3tuning::PerformanceEvaluator class requires the following inputs from the user: Task Learner Resampling Measure paradox::ParamSet It is similar to resample and benchmark with the additional requirement of a “Parameter Set” (paradox::ParamSet ) specifying the Hyperparameters of the given learner which should be optimized. An exemplary definition could looks as follows: library(mlr3tuning) task = mlr_tasks$get(&quot;iris&quot;) learner = mlr_learners$get(&quot;classif.rpart&quot;) resampling = mlr_resamplings$get(&quot;holdout&quot;) measures = mlr_measures$mget(&quot;classif.ce&quot;) param_set = paradox::ParamSet$new(params = list( paradox::ParamDbl$new(&quot;cp&quot;, lower = 0.001, upper = 0.1), paradox::ParamInt$new(&quot;minsplit&quot;, lower = 1, upper = 10))) pe = PerformanceEvaluator$new( task = task, learner = learner, resampling = resampling, measures = measures, param_set = param_set ) Evaluation of Single Parameter Settings Using the method .$eval(), the mlr3tuning::PerformanceEvaluator is able to tune a specific set of hyperparameters on the given inputs. The parameters have to be handed over wrapped in a data.table: pe$eval(data.table::data.table(cp = 0.05, minsplit = 5)) The results are stored in a BenchmarkResult class within the pe object. Note that this is the “bare bone” concept of using hyperparameters during Resampling. Usually you want to optimize the parameters in an automated fashion. 3.1.2 Tuning Hyperparameter Spaces Most often you do not want to only check the performance of fixed hyperparameter settings sequentially but optimize the outcome using different hyperparameter choices in an automated way. To achieve this, we need a definition of the search spaced that should be optimized. Let’s use again the space we defined in the introduction. paradox::ParamSet$new(params = list( paradox::ParamDbl$new(&quot;cp&quot;, lower = 0.001, upper = 0.1), paradox::ParamInt$new(&quot;minsplit&quot;, lower = 1, upper = 10))) To start the tuning, we still need to select how the optimization should take place - in other words, we need to choose the optimization algorithm. The following algorithms are currently implemented in mlr3: Grid Search (mlr3tuning::TunerGridSearch) Random Search (mlr3tuning::TunerRandomSearch) (Bergstra and Bengio 2012) Generalized Simulated Annealing (mlr3tuning::TunerGenSA) In this example we will use a simple “Grid Search”. Since we have only numeric parameters and specified the upper and lower bounds for the search space, mlr3tuning::TunerGridSearch will create a grid of equally-sized steps. By default, mlr3tuning::TunerGridSearch creates ten equal-sized steps. The number of steps can be changed with the resolution argument. In this example we use 15 steps and create a new class mlr3tuning::TunerGridSearch using the mlr3tuning::PerformanceEvaluator pe and the resolution. tuner_gs = TunerGridSearch$new(pe, resolution = 15) ## Error in assert_r6(terminator, &quot;Terminator&quot;): argument &quot;terminator&quot; is missing, with no default Oh! The error message tells us that we need to specify an addition argument called terminator. 3.1.3 Defining the Terminator What is a “Terminator”? The mlr3tuning::Terminator defines when the tuning should be stopped. This setting can have various instances: Terminate after a given time (mlr3tuning::TerminatorRuntime) Terminate after a given amount of iterations (mlr3tuning::TerminatorEvaluations) Terminate after a specific performance is reached (mlr3tuning::Performance) Often enough one termination criterion is not enough. For example, you will not know beforehand if all of your given evaluations will finish within a given amount of time. This highly depends on the Learner and the paradox::ParamSet given. However, you might not want to exceed a certain tuning time for each learner. In this case, it makes sense to combine both criteria using mlr3tuning::TerminatorMultiplexer. Tuning will stop as soon as one Terminator signals to be finished. In the following example we create two terminators and then combine them into one: tr = TerminatorRuntime$new(5) te = TerminatorEvaluations$new(max_evaluations = 50) tm = TerminatorMultiplexer$new(list(te, tr)) tm ## &lt;TerminatorMultiplexer&gt; (remaining: 50 evaluations, 5.000 secs) 3.1.4 Executing the Tuning Now that we have all required inputs (paradox::ParamSet, mlr3tuning::Terminator and the optimization algorithm), we can perform the hyperparameter tuning. The first step is to create the respective “Tuner” class, here mlr3tuning::TunerGridSearch. tuner_gs = TunerGridSearch$new(pe = pe, terminator = tm, resolution = 15) After it has been initialized, we can call its member function .$tune() to run the tuning. tuner_gs$tune() .$tune() simply performs a benchmark on the parameter values generated by the tuner and writes the results into a BenchmarkResult object which is stored in field .$bmr of the mlr3tuning::PerformanceEvaluator object that we passed to it. 3.1.5 Inspecting Results During the .$tune() call not only the BenchmarkResult output was written to the .$bmr slot of the mlr3tuning::PerformanceEvaluator but also the mlr3tuning::Terminator got updated. We can take a look by directly printing the mlr3tuning::Terminator object: print(tm) ## &lt;TerminatorMultiplexer&gt; (remaining: 0 evaluations, 1.977 secs) We can easily see that all evaluations were executed before the time limit kicked in. Now let’s take a closer look at the actual tuning result. It can be queried using .$tune_result() from the respective mlr3tuning::Tuner class that generated it. Internally, the function scrapes the data from the BenchmarkResult that was generated during tuning and stored in .$pe$bmr. tuner_gs$tune_result() ## $performance ## classif.ce ## 0.02 ## ## $values ## $values$xval ## [1] 0 ## ## $values$cp ## [1] 0.01514 ## ## $values$minsplit ## [1] 5 It returns the scored performance and the values of the optimized hyperparameters. Note that each measure “knows” if it was minimized or maximized during tuning: measures$classif.ce$minimize ## [1] TRUE A summary of the BenchmarkResult created by the tuning can be queried using the .$aggregate() function of the Tuner class. tuner_gs$aggregate() ## hash resample_result task_id learner_id resampling_id ## 1: 1046cfafb447d547 &lt;ResampleResult&gt; iris classif.rpart holdout ## 2: 4b9e6299135fe3bb &lt;ResampleResult&gt; iris classif.rpart holdout ## 3: dd90161e8899c5bc &lt;ResampleResult&gt; iris classif.rpart holdout ## 4: 3ffc7aa3cefa87ae &lt;ResampleResult&gt; iris classif.rpart holdout ## 5: 4b8b4f0d4e16ec4d &lt;ResampleResult&gt; iris classif.rpart holdout ## --- ## 147: d0205e5f596299f9 &lt;ResampleResult&gt; iris classif.rpart holdout ## 148: 360cb3454408b833 &lt;ResampleResult&gt; iris classif.rpart holdout ## 149: 86b432477b142b7a &lt;ResampleResult&gt; iris classif.rpart holdout ## 150: 5124d84647fbfdad &lt;ResampleResult&gt; iris classif.rpart holdout ## 151: 702245e64a2b094e &lt;ResampleResult&gt; iris classif.rpart holdout ## classif.ce xval cp minsplit ## 1: 0.08 0 0.050 5 ## 2: 0.04 0 0.001 1 ## 3: 0.04 0 0.001 2 ## 4: 0.04 0 0.001 3 ## 5: 0.04 0 0.001 4 ## --- ## 147: 0.04 0 0.100 6 ## 148: 0.04 0 0.100 7 ## 149: 0.04 0 0.100 8 ## 150: 0.04 0 0.100 9 ## 151: 0.04 0 0.100 10 Now the optimized hyperparameters can be used to create a new Learner and train it on the full dataset. task = mlr_tasks$get(&quot;iris&quot;) learner = mlr_learners$get(&quot;classif.rpart&quot;, param_vals = list( xval = tuner_gs$tune_result()$values$xval, cp = tuner_gs$tune_result()$values$cp) ) learner$train(task) 3.1.6 Automating the Tuning The steps shown above can be executed in a more convenient way using the mlr3tuning::AutoTuner class. This class gathers all the steps from above into a single call and uses the optimized hyperparameters from the tuning to create a new learner. Requirements: Task Learner Resampling Measure Parameter Set Terminator Tuning method Tuning settings (optional) task = mlr_tasks$get(&quot;iris&quot;) learner = mlr_learners$get(&quot;classif.rpart&quot;) resampling = mlr_resamplings$get(&quot;holdout&quot;) measures = mlr_measures$mget(&quot;classif.ce&quot;) param_set = paradox::ParamSet$new( params = list(paradox::ParamDbl$new(&quot;cp&quot;, lower = 0.001, upper = 0.1))) terminator = TerminatorEvaluations$new(5) at = mlr3tuning::AutoTuner$new(learner, resampling, measures = measures, param_set, terminator, tuner = TunerGridSearch, tuner_settings = list(resolution = 10L)) at$train(task) at$learner ## &lt;LearnerClassifRpart:classif.rpart&gt; ## Model: rpart ## Parameters: cp=0.034 ## Packages: rpart ## Predict Type: response ## Feature types: logical, integer, numeric, character, factor, ordered ## Properties: importance, missings, multiclass, selected_features, ## twoclass, weights Note that you can also pass the AutoTuner to resample() or benchmark(). By doing so, the AutoTuner will do its resampling for tuning on the training set of the respective split of the outer resampling. This is called nested resampling. To compare the tuned learner with the learner using its default, we can use benchmark(). bmr = benchmark(expand_grid(&quot;iris&quot;, list(at, &quot;classif.rpart&quot;), &quot;cv3&quot;)) bmr$aggregate(measures) ## hash resample_result task_id learner_id resampling_id ## 1: 79ddd57d7c841fa2 &lt;ResampleResult&gt; iris autotuner cv3 ## 2: cb190cdf29156238 &lt;ResampleResult&gt; iris classif.rpart cv3 ## classif.ce ## 1: 0.06 ## 2: 0.06 3.1.7 Summary Use PerformanceEvaluator$eval() for manual execution of parameters in Resampling Define a Tuner of your choice using a mlr3tuning::PerformanceEvaluator with the following inputs Learner Task Resampling paradox::ParamSet mlr3tuning::Terminator Inspect the tuning result using Tuner*$tune_result() Get a summary view of all runs based on the BenchmarkResult object created during tuning using Tuner*$aggregate() The AutoTuner class is a convenience wrapper that gathers all steps into one function References "],
["fs.html", "3.2 Feature Selection / Filtering", " 3.2 Feature Selection / Filtering Often, data sets include a large number of features. The technique of extracting a subset of relevant features is called “feature selection”. Feature selection can enhance the interpretability of the model, speed up the learning process and improve the learner performance. Different approaches exist to identify the relevant features. In the literature two different approaches exist: One is called “Filtering” and the other approach is often referred to as “feature subset selection” or “wrapper methods”. What are the differences (Chandrashekar and Sahin 2014)? Filter: An external algorithm computes a rank of the variables (e.g. based on the correlation to the response). Then, features are subsetted by a certain criteria, e.g. an absolute number or a percentage of the number of variables. The selected features will then be used to fit a model (with optional hyperparameters selected by tuning). This calculation is usually cheaper than “feature subset selection” in terms of computation time. Feature subset selection: Here, no ranking of features is done. Features are selected by a (random) subset of the data. Then, a model is fit and the performance is checked. This is done for a lot of feature combinations in a CV setting and the best combination is reported. This method is very computational intense as a lot of models are fitted. Also, strictly all these models would need to be tuned before the performance is estimated which would require an additional nested level in a CV setting. After all this, the selected subset of features is again fitted (with optional hyperparameters selected by tuning). There is also a third approach which can be attributed to the “filter” family: The embedded feature-selection methods of some Learner. Read more about how to use these in section embedded feature-selection methods. Ensemble filters built upon the idea of stacking single filter methods. These are not yet implemented. All feature selection related functionality is implemented via the extension package mlr3featsel. 3.2.1 Filters Filter methods assign an importance value to each feature. Based on these values the features can be ranked and a feature subset can be selected. There is a list of all implemented filter methods in the Appendix. 3.2.1.1 Calculating filter values Currently, only classification and regression tasks are supported. The first step it to create a new R object using the class of the desired filter method. Each object of class Filter has a .$calculate() method which calculates the ranking of the features. This method can be executed manually but is also run implicitly in the background if the actual filter functions (.$filter_nfeat(), .$filter_frac(), .$filter_cutoff()) are executed. All functions require a Task and return both the calculated filter values for all features and subset the supplied task: library(mlr3featsel) filter = FilterJMIM$new() task = mlr_tasks$get(&quot;iris&quot;) filter$calculate(task) as.data.table(filter) ## score feature method ## 1: 1.0401 Petal.Width jmim ## 2: 0.9894 Petal.Length jmim ## 3: 0.9881 Sepal.Length jmim ## 4: 0.8314 Sepal.Width jmim 3.2.1.2 Combining filter values You can also combine multiple results from different algorithms into one filter using .$combine(). filter2 = FilterDISR$new() task = mlr_tasks$get(&quot;iris&quot;) filter2$calculate(task) filter2$combine(filter) as.data.table(filter2) ## score feature method ## 1: 0.9894 Petal.Length disr ## 2: 0.7832 Sepal.Width disr ## 3: 0.5907 Sepal.Length disr ## 4: 0.3460 Petal.Width disr ## 5: 1.0401 Petal.Width jmim ## 6: 0.9894 Petal.Length jmim ## 7: 0.9881 Sepal.Length jmim ## 8: 0.8314 Sepal.Width jmim 3.2.1.3 Selecting a feature subset Instead of calculating the raw filter values, you can directly subset the task by using the member functions .$filter_nfeat(), .$filter_frac() and .$filter_cutoff(). These will also call .$calculate() implicitly in the background if it was not executed before. .$filter_nfeat(): Keep a certain absolute number (nfeat) of features with highest importance. .$filter_frac(): Keep a certain percentage (frac) of features with highest importance. .$filter_cutoff(): Keep all features whose importance exceeds a certain threshold value (cutoff). filter$filter_nfeat(task, 2) task$feature_names ## [1] &quot;Petal.Length&quot; &quot;Petal.Width&quot; 3.2.1.4 Custom construction of filters While Filter$new() creates a new R6 object of the respective filter, multiple parameters can be passed during construction. There are two types of hyperparameters that can be supplied: The parameters of the respective filter (if any) The generic hyperparameters nfeat, frac and thresh that apply to every filter. To get an overview of which are set by default, take a look at the ParamSet of a filter: FilterDISR$new()$param_set ## ParamSet: ## id class lower upper levels default value ## 1: k ParamInt 1 Inf 3 ## 2: threads ParamInt 0 Inf 0 ## 3: frac ParamDbl 0 1 &lt;NoDefault&gt; ## 4: cutoff ParamDbl -Inf Inf &lt;NoDefault&gt; ## 5: nfeat ParamInt 1 Inf &lt;NoDefault&gt; To set any of them during construction, use the param_vals argument. filter = FilterDISR$new(param_vals = list(nfeat = 2)) What has changed now? The created filter object already knows of the desired value of nfeat if its member function filter_nfeat() is called on a task. filter$filter_nfeat(task) ## Warning: Overwriting hyperparameter &#39;k&#39; with the value given in `$filter_*(). If you still pass a value for nfeat to .$filter_nfeat() it will be prioritized over the value set during construction. 3.2.2 Wrapper Methods Work in progress :) 3.2.3 Embedded Methods All Learner with the property “importance” come with integrated feature selection methods. You can find a list of all learners with this property in the Appendix. For some learners the desired filter method needs to be set during learner creation. For example, learner classif.ranger (in mlr3learners comes with multiple integrated methods. See the help page of ranger::ranger. To use method “impurity”, you need to set it via the param_vals argument: library(mlr3learners) lrn = mlr_learners$get(&quot;classif.ranger&quot;, param_vals = list(importance = &quot;impurity&quot;)) Now you can use the mlr3featsel::FilterEmbedded class for algorithm-embedded methods to filter a Task. task = mlr_tasks$get(&quot;iris&quot;) filter = FilterEmbedded$new(learner = lrn) filter$calculate(task) head(as.data.table(filter), 3) ## score feature method ## 1: 45.30 Petal.Length embedded ## 2: 41.29 Petal.Width embedded ## 3: 10.31 Sepal.Length embedded 3.2.4 Ensemble Methods Work in progress :) References "],
["nested-resampling.html", "3.3 Nested Resampling", " 3.3 Nested Resampling 3.3.1 Introduction In order to obtain unbiased performance estimates for a learners, all parts of the model building (preprocessing and model selection steps) should be included in the resampling, i.e., repeated for every pair of training/test data. For steps that themselves require resampling like hyperparameter tuning or feature-selection (via the wrapper approach) this results in two nested resampling loops. The graphic above illustrates nested resampling for parameter tuning with 3-fold cross-validation in the outer and 4-fold cross-validation in the inner loop. In the outer resampling loop, we have three pairs of training/test sets. On each of these outer training sets parameter tuning is done, thereby executing the inner resampling loop. This way, we get one set of selected hyperparameters for each outer training set. Then the learner is fitted on each outer training set using the corresponding selected hyperparameters and its performance is evaluated on the outer test sets. In mlr3, you can get nested resampling for free without programming any looping by using the mlr3tuning::AutoTuner class. This works as follows: Generate a wrapped Learner via class mlr3tuning::AutoTuner or mlr3featsel::AutoSelect (not yet implemented). Specify all required settings - see section “Automating the Tuning” for help. Call function resample() or benchmark() with the created Learner. You can freely combine different inner and outer resampling strategies. A common setup is prediction and performance evaluation on a fixed outer test set. This can be achieved by passing the Resampling strategy (mlr_resamplings$get(\"holdout\")) as the outer resampling instance to either resample() or benchmark(). The inner resampling strategy could be a cross-validation one (mlr_resamplings$get(\"cv\")) as the sizes of the outer training sets might differ. Per default, the inner resample description is instantiated once for every outer training set. Nested resampling is computationally expensive. For this reason in the examples shown below we use relatively small search spaces and a low number of resampling iterations. In practice, you normally have to increase both. As this is computationally intensive you might want to have a look at section parallelization. 3.3.2 Execution To optimize hyperparameters or conduct features-selection in a nested resampling you need to create learners using either the mlr3tuning::AutoTuner class, or the mlr3featsel::AutoSelect class (not yet implemented). We use the example from section “Automating the Tuning” and pipe the resulting learner into a resample() call. task = mlr_tasks$get(&quot;iris&quot;) learner = mlr_learners$get(&quot;classif.rpart&quot;) resampling = mlr_resamplings$get(&quot;holdout&quot;) measures = mlr_measures$mget(&quot;classif.ce&quot;) param_set = paradox::ParamSet$new( params = list(paradox::ParamDbl$new(&quot;cp&quot;, lower = 0.001, upper = 0.1))) terminator = TerminatorEvaluations$new(5) at = mlr3tuning::AutoTuner$new(learner, resampling, measures = measures, param_set, terminator, tuner = TunerGridSearch, tuner_settings = list(resolution = 10L)) at$store_bmr = TRUE Now construct the resample() call: resampling_outer = mlr_resamplings$get(&quot;cv3&quot;) rr = resample(task = task, learner = at, resampling = resampling_outer) 3.3.3 Evaluation With the created ResampleResult we can now inspect the executed resampling iterations more closely. See also section Resampling for more detailed information about ResampleResult objects. For example, we can query the aggregated performance result: rr$aggregate() ## classif.ce ## 0.06 We can also query the tuning result of any learner using the $tune_path field of the AutoTuner class stored in the ResampleResult container rr. Note: This only works if store_bmr was set to TRUE in the AutoTuner object. rr$learners[[1]]$tune_path ## hash resample_result task_id learner_id resampling_id ## 1: e480d7ffcf469368 &lt;ResampleResult&gt; iris classif.rpart holdout ## 2: d3b885b42a916158 &lt;ResampleResult&gt; iris classif.rpart holdout ## 3: 6966c938c2474e98 &lt;ResampleResult&gt; iris classif.rpart holdout ## 4: 5b3f7670983618e8 &lt;ResampleResult&gt; iris classif.rpart holdout ## 5: da6c65d3b0a26d76 &lt;ResampleResult&gt; iris classif.rpart holdout ## 6: e91a72d901c4df3e &lt;ResampleResult&gt; iris classif.rpart holdout ## 7: bf254f298f51c7fa &lt;ResampleResult&gt; iris classif.rpart holdout ## 8: 57bf8504b3e167d5 &lt;ResampleResult&gt; iris classif.rpart holdout ## 9: a73b7d503257411e &lt;ResampleResult&gt; iris classif.rpart holdout ## 10: 0419fe89323d5494 &lt;ResampleResult&gt; iris classif.rpart holdout ## classif.ce cp ## 1: 0.06061 0.001 ## 2: 0.06061 0.012 ## 3: 0.06061 0.023 ## 4: 0.06061 0.034 ## 5: 0.06061 0.045 ## 6: 0.06061 0.056 ## 7: 0.06061 0.067 ## 8: 0.06061 0.078 ## 9: 0.06061 0.089 ## 10: 0.06061 0.100 Check for any errors in the folds during execution (if there is not output, warnings or errors recorded, this is an empty data.table(): rr$errors ## Empty data.table (0 rows and 2 cols): iteration,warning Or take a look at the confusion matrix of the joined predictions: rr$prediction$confusion ## truth ## response setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 45 4 ## virginica 0 5 46 "],
["pipelines.html", "4 Pipelines", " 4 Pipelines mlr3pipelines is a dataflow programming toolkit for machine learning in R utilizing the mlr3pipelines package. A more in-deptth and technically oriented vignette can be found in the mlr3pipeline vignette. Machine learning workflows can be written as directed “Graphs”/“Pipelines” that represent data flows between preprocessing, model fitting, and ensemble learning units in an expressive and intuitive language. We will most often use the term “Graph” in this manual but it can interchangeably be used with “pipeline” or “workflow”. Single computational steps can be represented as so-called PipeOps, which can then be connected with directed edges in a Graph. The scope of mlr3pipelines is still growing. Currently supported features are: Simple data manipulation and preprocessing operations, e.g. PCA, feature filtering, imputation Task subsampling for speed and outcome class imbalance handling mlr3 Learner operations for prediction and stacking Ensemble methods and aggregation of predictions Additionally, we implement several meta operators that can be used to construct powerful pipelines: - Simultaneous path branching (data going both ways) - Alternative path branching (data going one specific way, controlled by hyperparameters) An extensive introduction to creating custom PipeOps (PO’s) can be found in the technical introduction. Using methods from mlr3tuning, it is even possible to simultaneously optimize parameters of multiple processing units. A predecessor to this package is the mlrCPO package, which works with mlr 2.x. Other packages that provide, to varying degree, some preprocessing functionality or machine learning domain specific language, are the caret package and the related recipies project, and the dplyr package. "],
["the-building-blocks-pipeops.html", "4.1 The Building Blocks: PipeOps", " 4.1 The Building Blocks: PipeOps The building blocks of mlr3pipelines are PipeOp-objects (PO). They can be constructed directly using PipeOp&lt;NAME&gt;$new(), but the recommended way is to retrieve them from the mlr_pipeops dictionary. library(&quot;mlr3pipelines&quot;) as.data.table(mlr_pipeops) #&gt; key packages input.num output.num input.type.train #&gt; 1: apply 1 1 Task #&gt; 2: backuplearner 2 1 NULL,Task #&gt; 3: balancesample 1 1 Task #&gt; 4: branch 1 NA * #&gt; 5: chunk 1 NA Task #&gt; 6: copy 1 NA * #&gt; 7: encode stats 1 1 Task #&gt; 8: featureunion NA 1 Task #&gt; 9: filter 1 1 Task #&gt; 10: impute 1 1 Task #&gt; 11: learner 1 1 Task #&gt; 12: learner_cv 1 1 Task #&gt; 13: majorityvote NA 1 NULL #&gt; 14: modelavg NA 1 NULL #&gt; 15: mutate 1 1 Task #&gt; 16: null 1 1 * #&gt; 17: pca 1 1 Task #&gt; 18: scale 1 1 Task #&gt; 19: select 1 1 Task #&gt; 20: subsample 1 1 Task #&gt; 21: unbranch NA 1 * #&gt; key packages input.num output.num input.type.train #&gt; input.type.predict output.type.train output.type.predict #&gt; 1: Task Task Task #&gt; 2: Prediction,Task NULL Prediction #&gt; 3: Task Task Task #&gt; 4: * * * #&gt; 5: Task Task Task #&gt; 6: * * * #&gt; 7: Task Task Task #&gt; 8: Task Task Task #&gt; 9: Task Task Task #&gt; 10: Task Task Task #&gt; 11: Task NULL Prediction #&gt; 12: Task Task Task #&gt; 13: PredictionClassif NULL PredictionClassif #&gt; 14: PredictionRegr NULL PredictionRegr #&gt; 15: Task Task Task #&gt; 16: * * * #&gt; 17: Task Task Task #&gt; 18: Task Task Task #&gt; 19: Task Task Task #&gt; 20: Task Task Task #&gt; 21: * * * #&gt; input.type.predict output.type.train output.type.predict Single POs can be created using mlr_pipeops$get(&lt;name&gt;). pca = mlr_pipeops$get(&quot;pca&quot;) Some POs require additional arguments for construction. learner = mlr_pipeops$get(&quot;learner&quot;) # Error in cast_from_dict(learner, &quot;Learner&quot;, mlr_learners, clone, FALSE) : argument &quot;learner&quot; is missing, with no default learner = mlr_pipeops$get(&quot;learner&quot;, mlr_learners$get(&quot;classif.ranger&quot;)) Hyperparameters of POs can be set through the param_vals argument. Here we set the fraction of features for a filter. filter = mlr_pipeops$get(&quot;filter&quot;, filter = mlr3featsel::FilterVariance$new(), param_vals = list(filter.frac = 0.5)) "],
["pipe-operator.html", "4.2 The Pipeline Operator: %&gt;&gt;%", " 4.2 The Pipeline Operator: %&gt;&gt;% Although it is possible to create intricate Graphs with edges going all over the place (as long as no loops are introduced), there is usually a clear direction of flow between “layers” in the Graph. It is therefore convenient to build up a Graph from layers, which can be done using the %&gt;&gt;% (“double-arrow”) operator. It takes either a PipeOp or a Graph on each of its sides and connects all of the outputs of its left-hand side to one of the inputs each of its right-hand side–the number of inputs therefore must match the number of outputs. gr = mlr_pipeops$get(&quot;scale&quot;) %&gt;&gt;% mlr_pipeops$get(&quot;pca&quot;) gr$plot(html = TRUE) %&gt;% visInteraction(zoomView = FALSE) # disable zoom "],
["pipe-nodes-edges-graphs.html", "4.3 Nodes, Edges and Graphs", " 4.3 Nodes, Edges and Graphs POs are combined into Graph s. The manual way (= hard way) to construct a Graph is to create an empty one first, fill it with POs, and then connect edges between the POs. POs are identified by their $id. Note that the operations all modify the object in-place and return the object itself, so multiple modifications can be chained. For this example we use the “pca” PO defined above and a new PO named “mutate”. The latter creates a new feature from existing variables. mutate = mlr_pipeops$get(&quot;mutate&quot;) graph = Graph$new()$ add_pipeop(mutate)$ add_pipeop(filter)$ add_edge(&quot;mutate&quot;, &quot;variance&quot;) # add connection mutate -&gt; filter The much quicker way is to use the %&gt;&gt;% operator to chain POs or Graph s. The same result as above can be achieved by doing graph = mutate %&gt;&gt;% filter Now the Graph can be inspected using its $plot() function: graph$plot(html = TRUE) %&gt;% visInteraction(zoomView = FALSE) # disable zoom Chaining multiple POs of the same kind If multiple POs of the same kind should be chained it is necessary to change the id to avoid name clashes. This can be done by either accessing the $id slot or during construction. graph$add_pipeop(mlr_pipeops$get(&quot;pca&quot;)) graph$add_pipeop(mlr_pipeops$get(&quot;pca&quot;, id = &quot;pca2&quot;)) "],
["pipe-modeling.html", "4.4 Modeling", " 4.4 Modeling The main purpose of a Graph is to build combined preprocessing and model fitting pipelines that can be used as mlr3 Learner. In the following we chain two preprocessing tasks mutate (creation of a new feature) filter (filtering the dataset) and then chain a PO learner to train and predict on the modified dataset. graph = mutate %&gt;&gt;% filter %&gt;&gt;% mlr_pipeops$get(&quot;learner&quot;, learner = mlr_learners$get(&quot;classif.rpart&quot;)) Until here we defined the main pipeline stored in Graph. Now we can train and predict the pipeline. task = mlr_tasks$get(&quot;iris&quot;) graph$train(task) #&gt; $classif.rpart.output #&gt; NULL graph$predict(task) #&gt; $classif.rpart.output #&gt; &lt;PredictionClassif&gt; for 150 observations: #&gt; row_id truth response #&gt; 1: 1 setosa setosa #&gt; 2: 2 setosa setosa #&gt; 3: 3 setosa setosa #&gt; --- #&gt; 148: 148 virginica virginica #&gt; 149: 149 virginica virginica #&gt; 150: 150 virginica virginica Rather than calling $train() and $predict() manually, we can put the pipeline Graph into a GraphLearner object. A GraphLearner encapsulates the whole pipeline (including the preprocessing steps) and can be put into resample() or benchmark() . If you are familiar with the old mlr package, this is the equivalent of all the make*Wrapper() functions. The pipeline being encapsulated (here Graph ) must always produce a Prediction with its $predict() call, so it will probably contain at least one PipeOpLearner . glrn = GraphLearner$new(graph) This learner can be used for model fitting, resampling, benchmarking, and tuning. resample(task, glrn, &quot;cv3&quot;) #&gt; &lt;ResampleResult&gt; of 3 iterations #&gt; Task: iris #&gt; Learner: mutate.variance.classif.rpart 4.4.1 Setting Hyperparameters Individual POs offer hyperparameters because they contain $param_set slots that can be read and written from $param_set$values (via the paradox package). The parameters get passed down to the Graph, and finally to the GraphLearner . This makes it not only possible to easily change change the behavior of a Graph / GraphLearner and try different settings manually, but also to perform tuning using the mlr3tuning package. glrn$param_set$values$variance.filter.frac = 0.25 resample(task, glrn, &quot;cv3&quot;) #&gt; &lt;ResampleResult&gt; of 3 iterations #&gt; Task: iris #&gt; Learner: mutate.variance.classif.rpart 4.4.2 Tuning If you are unfamiliar with tuning in mlr3 yet, we recommend to take a look at the section about tuning first. Here we define a ParamSet for the “rpart” learner and the “variance” filter which should be optimized during tuning. library(&quot;paradox&quot;) ps = ParamSet$new(list( ParamDbl$new(&quot;classif.rpart.cp&quot;, lower = 0, upper = 0.05), ParamDbl$new(&quot;variance.filter.frac&quot;, lower = 0.25, upper = 1) )) After having defined the PerformanceEvaluator, a random search with 10 iterations is created. For the inner resampling, we are simply doing holdout (single split into train/test) to keep the runtimes reasonable. library(&quot;mlr3tuning&quot;) pe = PerformanceEvaluator$new(task, glrn, &quot;holdout&quot;, &quot;classif.ce&quot;, ps) tuner = TunerRandomSearch$new(pe, TerminatorEvaluations$new(10)) tuner$tune() The tuning result can be inspected using the $tune_result() method. tuner$tune_result() #&gt; $performance #&gt; classif.ce #&gt; 0.08 #&gt; #&gt; $values #&gt; $values$mutate.mutation #&gt; named list() #&gt; #&gt; $values$mutate.env #&gt; &lt;environment: R_GlobalEnv&gt; #&gt; #&gt; $values$mutate.delete_originals #&gt; [1] FALSE #&gt; #&gt; $values$variance.filter.frac #&gt; [1] 0.4311 #&gt; #&gt; $values$variance.na.rm #&gt; [1] TRUE #&gt; #&gt; $values$classif.rpart.cp #&gt; [1] 0.02458 #&gt; #&gt; $values$classif.rpart.xval #&gt; [1] 0 "],
["pipe-nonlinear.html", "4.5 Non-Linear Graphs", " 4.5 Non-Linear Graphs The Graphs seen so far all have a linear structure. Some POs may have multiple input or output channels. These make it possible to create non-linear Graphs with alternative paths taken by the data. Possible types are Branching: Splitting of a node into several paths, useful for example when comparing multiple feature-selection methods (pca, filters). Only one path will be executed. Copying: Splitting of a node into several paths, all paths will be executed (sequentially). Parallel execution is not yet supported. Stacking: Single graphs are stacked onto each other, i.e. the output of one Graph is the input for another. In machine learning this means that the prediction of one Graph is used as input for another Graph 4.5.1 Branching &amp; Copying The PipeOpBranch and PipeOpUnbranch POs make it possible to specify multiple alternative paths. Only one is actually executed, the others are ignored. The active path is determined by a hyperparameter. This concept makes it possible to tune alternative preprocessing paths (or learner models). PipeOp(Un)Branch is initialized either with the number of branches, or with a character-vector indicating the names of the branches. If names are given, the “branch-choosing” hyperparameter becomes more readable. In the following, we set three options Doing nothing (“null”) Applying a PCA Scaling the data It is important to “unbranch” again after “branching”, so that the outputs are merged into one result objects. In the following we first create the branched graph and then show what happens if the “unbranching” is not applied. graph = mlr_pipeops$get(&quot;branch&quot;, c(&quot;null&quot;, &quot;pca&quot;, &quot;scale&quot;)) %&gt;&gt;% gunion(list( mlr_pipeops$get(&quot;null&quot;, id = &quot;null1&quot;), mlr_pipeops$get(&quot;pca&quot;), mlr_pipeops$get(&quot;scale&quot;) )) Without “unbranching”: graph$plot(html = TRUE) %&gt;% visInteraction(zoomView = FALSE) # disable zoom With “unbranching”: (graph %&gt;&gt;% mlr_pipeops$get(&quot;unbranch&quot;, c(&quot;null&quot;, &quot;pca&quot;, &quot;scale&quot;)))$plot(html = TRUE) %&gt;% visInteraction(zoomView = FALSE) # disable zoom 4.5.2 Model Ensembles We can leverage the different operations presented to connect POs to form powerful graphs. Before we go into details, we split the task into train and test indices. task = mlr_tasks$get(&quot;iris&quot;) train.idx = sample(seq_len(task$nrow), 120) test.idx = setdiff(seq_len(task$nrow), train.idx) 4.5.2.1 Bagging We first examine Bagging introduced by (Breiman 1996). The basic idea is to create multiple predictors and then aggregate those to a single, more powerful predictor. “… multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets” (Breiman 1996) Bagging then aggregates a set of predictors by averaging (regression) or majority vote (classification). The idea behind bagging is, that a set of weak, but different predictors can be combined in order to arrive at a single, better predictor. We can achieve this by downsampling our data before training a learner, repeating this for say 10 times and then performing a majority vote on the predictions. First, we create a simple pipeline, that uses PipeOpSubsample before a PipeOpLearner is trained: single_pred = PipeOpSubsample$new(param_vals = list(frac = 0.7)) %&gt;&gt;% PipeOpLearner$new(mlr_learners$get(&quot;classif.rpart&quot;)) We can now copy this operation 10 times using greplicate . pred_set = greplicate(single_pred, 10L) Afterwards we need to aggregate the 10 pipelines to form a single model: bagging = pred_set %&gt;&gt;% PipeOpMajorityVote$new(innum = 10L) and plot again to see what happens: bagging$plot(html = TRUE) %&gt;% visInteraction(zoomView = FALSE) # disable zoom This pipeline can again be used in conjunction with GraphLearner in order for Bagging to be used like a Learner. baglrn = GraphLearner$new(bagging) baglrn$train(task, train.idx) baglrn$predict(task, test.idx) #&gt; &lt;PredictionClassif&gt; for 30 observations: #&gt; row_id truth response #&gt; 1: 6 setosa setosa #&gt; 2: 9 setosa setosa #&gt; 3: 12 setosa setosa #&gt; --- #&gt; 28: 128 virginica virginica #&gt; 29: 137 virginica virginica #&gt; 30: 147 virginica virginica In conjunction with different Backends, this can be a very powerful tool. In cases when the data does not fully fit in memory, we can obtain a fraction of the data for each learner from a DataBackend and then aggregate predictions over all learners. 4.5.2.2 Stacking Stacking (Wolpert 1992) is another technique that can improve model performance. The basic idea behind stacking is the use of predictions from one model as features for a subsequent model to possibly improve performance. As an example we can train a decision tree and use the predictions from this model in conjunction with the original features in order to train an additional model on top. In order to limit overfitting, we additionally do not predict on the original predictions of the learner, but instead on out-of-bag predictions. To do all this, we can use PipeOpLearnerCV . PipeOpLearnerCV performs nested cross-validation on the training data, fitting a model in each fold. Each of the models is then used to predict on the out-of-fold data. As a result, we obtain predictions for every data point in our input data. We first create a “level 0” learner, which is used to extract a lower level prediction. We additionally clone() the learner object to obtain a copy of the learner, and set a custom id for the PipeOp . lrn = mlr_learners$get(&quot;classif.rpart&quot;) lrn_0 = PipeOpLearnerCV$new(lrn$clone()) lrn_0$id = &quot;rpart_cv&quot; Additionally, we use a PipeOpNULL in parallel to the “level 0” learner, in order to send the unchanged Task to the next level, where it is then combined with the predictions from our decision tree learner. level_0 = gunion(list(lrn_0, PipeOpNULL$new())) Afterwards, we want to concatenate the predictions from PipeOpLearnerCV and the original Task using PipeOpFeatureUnion . combined = level_0 %&gt;&gt;% PipeOpFeatureUnion$new(2) We can now train another learner on top of the combined features. stack = combined %&gt;&gt;% PipeOpLearner$new(lrn$clone()) stack$plot(html = TRUE) %&gt;% visInteraction(zoomView = FALSE) # disable zoom stacklrn = GraphLearner$new(stack) stacklrn$train(task, train.idx) stacklrn$predict(task, test.idx) #&gt; &lt;PredictionClassif&gt; for 30 observations: #&gt; row_id truth response #&gt; 1: 6 setosa setosa #&gt; 2: 9 setosa setosa #&gt; 3: 12 setosa setosa #&gt; --- #&gt; 28: 128 virginica virginica #&gt; 29: 137 virginica virginica #&gt; 30: 147 virginica virginica In this vignette, we showed a very simple usecase for stacking. In many real-world applications, stacking is done for multiple levels and on multiple representations of the dataset. On a lower level, different preprocessing methods can be defined in conjunction with several learners. On a higher level, we can then combine those predictions in order to form a very powerful model. 4.5.2.3 Multilevel Stacking In order to showcase the power of mlr3pipelines, we will show a more complicated stacking example. In this case, we train a glmnet and 2 different rpart models (some transform its inputs using PipeOpPCA ) on our task in the “level 0” and concatenate them with the original features (via PipeOpNULL ). The result is then passed on to “level 1”, where we copy the concatenated features 3 times and put this task into a rpart and a glmnet model. Additionally, we keep a version of the “level 0” output (via PipeOpNULL ) and pass this on to “level 2”. In “level 2” we simply concatenate all “level 1” outputs and train a final decision tree. rprt = mlr_learners$get(&quot;classif.rpart&quot;) rprt$predict_type = &quot;prob&quot; glmn = mlr_learners$get(&quot;classif.glmnet&quot;) glmn$predict_type = &quot;prob&quot; # Create Learner CV Operators lrn_0 = PipeOpLearnerCV$new(rprt, id = &quot;rpart_cv_1&quot;) lrn_0$values$maxdepth = 5L lrn_1 = PipeOpPCA$new(id = &quot;pca1&quot;) %&gt;&gt;% PipeOpLearnerCV$new(rprt, id = &quot;rpart_cv_2&quot;) lrn_1$values$maxdepth = 1L lrn_2 = PipeOpPCA$new(id = &quot;pca2&quot;) %&gt;&gt;% PipeOpLearnerCV$new(glmn) # Union them with a PipeOpNULL to keep original features level_0 = gunion(list(lrn_0, lrn_1,lrn_2, PipeOpNULL$new(id = &quot;NULL1&quot;))) # Cbind the output 3 times, train 2 learners but also keep level # 0 predictions level_1 = level_0 %&gt;&gt;% PipeOpFeatureUnion$new(4) %&gt;&gt;% PipeOpCopy$new(3) %&gt;&gt;% gunion(list( PipeOpLearnerCV$new(rprt, id = &quot;rpart_cv_l1&quot;), PipeOpLearnerCV$new(glmn, id = &quot;glmnt_cv_l1&quot;), PipeOpNULL$new(id = &quot;NULL_l1&quot;) )) # Cbind predicitions, train a final learner. level_2 = level_1 %&gt;&gt;% PipeOpFeatureUnion$new(3, id = &quot;u2&quot;) %&gt;&gt;% PipeOpLearner$new(rprt, id = &quot;rpart_l2&quot;) # Plot the resulting graph level_2$plot(html = TRUE) %&gt;% visInteraction(zoomView = FALSE) # disable zoom task = mlr_tasks$get(&quot;iris&quot;), lrn = GraphLearner$new(level_2) lrn$ train(task, train.idx)$ predict(task, test.idx)$ score() References "],
["pipe-special-ops.html", "4.6 Special Operators", " 4.6 Special Operators This section introduce some special operators, that might be useful in many applications. 4.6.1 Imputation: PipeOpImpute An often occurring setting is the imputation of missing data. Imputation methods range from relatively simple imputation using either mean, median or histograms to way more involved methods including using machine learning algorithms in order to predict missing values. The following PipeOp imputes numeric values from a histogram, adds a new level for factors and additionally adds a column marking whether a value for a given feature was missing or not. poi = PipeOpImpute$new(param_vals = list(method_num = &quot;hist&quot;, method_fct = &quot;sample&quot;, add_dummy = &quot;all&quot;)) A learner can thus be equipped with automatic imputation of missing values by adding an imputation Pipeop. polrn = PipeOpLearner$new(mlr_learners$get(&quot;classif.rpart&quot;)) lrn = GraphLearner$new(graph = poi %&gt;&gt;% polrn) 4.6.2 Feature Engineering: PipeOpMutate New features can be added or computed from a task using PipeOpMutate . The operator evaluates one or multiple expressions provided in an alist. In this example, we compute some new features on top of the iris task and add them to the data. pom = PipeOpMutate$new() # Define a set of mutations mutations = alist( Sepal.Sum = Sepal.Length + Sepal.Width, Petal.Sum = Petal.Length + Petal.Width, Sepal.Petal.Ratio = (Sepal.Length / Petal.Length) ) pom$param_set$values$mutation = mutations If outside data is required, we can make use of the env parameter and additionally provide an environment, where expressions are evaluated (env defaults to .GlobalEnv). 4.6.3 Training on data subsets: PipeOpChunk In cases, where data is too big to fit into the machine’s memory, an often-used technique is to split the data into several parts, train on each part of the data and afterwards aggregate the models. In this example, we split our data into 4 parts using PipeOpChunk . Additionally, we create 4 PipeOpLearner POS, which are then trained on each split of the data. chks = PipeOpChunk$new(4) lrns = greplicate(PipeOpLearner$new(mlr_learners$get(&quot;classif.rpart&quot;)), 4) Afterwards we can use PipeOpMajorityVote to aggregate the predictions from the 4 different models into a new one. mjv = PipeOpMajorityVote$new(4) We can now connect the different operators and visualize the full graph: pipeline = chks %&gt;&gt;% lrns %&gt;&gt;% mjv pipeline$plot(html = TRUE) %&gt;% visInteraction(zoomView = FALSE) # disable zoom pipelrn = GraphLearner$new(pipeline) pipelrn$train(task, train.idx)$ predict(task, train.idx)$ score() #&gt; classif.ce #&gt; 0.08333 4.6.4 Feature Selection: PipeOpFilter , PipeOpSelect mlr3featsel contains many different Filters that can be used to select features for subsequent learners. This is often required when the data has a large amount of features. PipeOpFilter$new(mlr3featsel::FilterInformationGain$new()) #&gt; PipeOp: &lt;information_gain&gt; (not trained) #&gt; values: &lt;equal=TRUE&gt; #&gt; Input channels &lt;name [train type, predict type]&gt;: #&gt; input [Task,Task] #&gt; Output channels &lt;name [train type, predict type]&gt;: #&gt; output [Task,Task] How many features to keep can be set using filter_nfeat, filter_frac and filter_cutoff. Filters can be selected / de-selected by name using PipeOpSelect . "],
["technical.html", "5 Technical ", " 5 Technical "],
["parallelization.html", "5.1 Parallelization", " 5.1 Parallelization mlr3 uses the future backends for parallelization. Make sure you have installed the required packages future and future.apply: mlr3 is capable of parallelizing a variety of different scenarios. One of the most used cases is to parallelize the Resampling iterations. See Section Resampling for a detailed introduction to resampling. In the following, we will use the spam task and a simple classification tree (\"classif.rpart\") to showcase parallelization. We use the future package to parallelize the resampling by selecting a backend via the function future::plan(). We use the \"multiprocess\" backend here which uses threads on UNIX based systems and a “Socket” cluster on Windows. future::plan(&quot;multiprocess&quot;) task = mlr_tasks$get(&quot;spam&quot;) learner = mlr_learners$get(&quot;classif.rpart&quot;) resampling = mlr_resamplings$get(&quot;subsampling&quot;) time = Sys.time() resample(task, learner, resampling) Sys.time() - time By default all CPUs of your machine are used unless you specify argument workers in future::plan(). On most systems you should see a decrease in the reported elapsed time. On some systems (e.g. Windows), the overhead for parallelization is quite large though. Therefore, it is advised to only enable parallelization for resamplings where each iteration runs at least 10s. Choosing the parallelization level If you have are transitioning from mlr you might be used to selecting different parallelization levels, e.g. for resampling, benchmarking or tuning. In mlr3 this is no longer required All kind of events are rolled out on the same level - no need to decide whether you want to parallelize the tuning OR the resampling. Just lean back and let the machine do the work :-) "],
["error-handling.html", "5.2 Error Handling", " 5.2 Error Handling To demonstrate how to properly deal with misbehaving learners, mlr3 ships with the learner classif.debug: task = mlr_tasks$get(&quot;spam&quot;) learner = mlr_learners$get(&quot;classif.debug&quot;) print(learner) #&gt; &lt;LearnerClassifDebug:classif.debug&gt; #&gt; Model: - #&gt; Parameters: list() #&gt; Packages: - #&gt; Predict Type: response #&gt; Feature types: logical, integer, numeric, character, factor, ordered #&gt; Properties: missings, multiclass, twoclass This learner comes with special hyperparameters that let us control What conditions should be signaled (message, warning, error), and during which stage the conditions should be signaled (train or predict). learner$param_set #&gt; ParamSet: #&gt; id class lower upper levels default value #&gt; 1: message_train ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; #&gt; 2: message_predict ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; #&gt; 3: warning_train ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; #&gt; 4: warning_predict ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; #&gt; 5: error_train ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; #&gt; 6: error_predict ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; #&gt; 7: segfault_train ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; #&gt; 8: segfault_predict ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; #&gt; 9: predict_missing ParamDbl 0 1 0 #&gt; 10: save_tasks ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; #&gt; 11: x ParamDbl 0 1 &lt;NoDefault&gt; Alternatively, we can tell the Learner to provoke a segfault which tears down the complete R session. With its default settings, it will do nothing special: it learns a random label which is used to create constant predictions. 5.2.1 Encapsulation By default,mlr3 does not catch conditions such as warnings or errors. Thus, the exception raised by the debug learner stops the execution allowing us to traceback() the error: task = mlr_tasks$get(&quot;spam&quot;) learner = mlr_learners$get(&quot;classif.debug&quot;) learner$param_set$values = list(error_train = TRUE) learner$train(task) #&gt; Error in learner$train_internal(task = task): Error from classif.debug-&gt;train() The learner execution can be encapsulated though. With encapsulation, exceptions do not stop the program flow and any output is logged to the learner instead of just printed to the console. One way to encapsulate the execution is provided by the package evaluate. The encapsulation can be enabled via mlr_control(): task = mlr_tasks$get(&quot;spam&quot;) learner = mlr_learners$get(&quot;classif.debug&quot;) learner$param_set$values = list(warning_train = TRUE, error_train = TRUE) ctrl = mlr_control(encapsulate_train = &quot;evaluate&quot;) learner$train(task, ctrl = ctrl) learner$log #&gt; stage class msg #&gt; 1: train warning Warning from classif.debug-&gt;train() #&gt; 2: train error Error from classif.debug-&gt;train() learner$errors #&gt; [1] &quot;Error from classif.debug-&gt;train()&quot; You can also enable the encapsulation for the predict step of a learner by setting encapsulate_predict in mlr_control(). Another possibility to encapsulate is by running everything in a callr session. callr spawns a new R process, and thus even guards the session from segfaults. On the downside, starting new processes comes with a computational overhead. ctrl = mlr_control(encapsulate_train = &quot;callr&quot;) task = mlr_tasks$get(&quot;spam&quot;) learner = mlr_learners$get(&quot;classif.debug&quot;) learner$param_set$values = list(segfault_train = TRUE) learner$train(task = task, ctrl = ctrl) learner$errors #&gt; [1] &quot;callr exited with status -11&quot; Without a model, it is not possible to predict: learner$predict(task) #&gt; Error: No model available, call `train()` first 5.2.2 Fallback learners Fallback learners have the purpose to continue with the computation in cases where a Learner or a Measure are misbehaving in some sense. Some typical examples include: The learner fails to fit a model during training. This can happen if some convergence criterion is not met or the learner ran out of memory. The learner fails to predict for some or all observations. A typical case could be new factor levels in the test data which the model cannot handle. The fallback learner from the package mlr3pipelines can be used for these scenarios. This is still work in progress. "],
["backends.html", "5.3 Database Backends", " 5.3 Database Backends In mlr3, Tasks store their data in an abstract data format, the DataBackend. The default backend uses data.table via the DataBackendDataTable as an in-memory data base. For larger data, or when working with many tasks in parallel, it can be advantageous to interface an out-of-memory data. We use the excellent R package dbplyr which extends dplyr to work on many popular data bases like MariaDB, PostgreSQL or SQLite. 5.3.1 Example Data To generate a halfway realistic scenario, we use the NYC flights data set from package nycflights13: # load data requireNamespace(&quot;DBI&quot;) #&gt; Loading required namespace: DBI requireNamespace(&quot;RSQLite&quot;) #&gt; Loading required namespace: RSQLite requireNamespace(&quot;nycflights13&quot;) #&gt; Loading required namespace: nycflights13 data(&quot;flights&quot;, package = &quot;nycflights13&quot;) str(flights) #&gt; Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 336776 obs. of 19 variables: #&gt; $ year : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... #&gt; $ month : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ day : int 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ dep_time : int 517 533 542 544 554 554 555 557 557 558 ... #&gt; $ sched_dep_time: int 515 529 540 545 600 558 600 600 600 600 ... #&gt; $ dep_delay : num 2 4 2 -1 -6 -4 -5 -3 -3 -2 ... #&gt; $ arr_time : int 830 850 923 1004 812 740 913 709 838 753 ... #&gt; $ sched_arr_time: int 819 830 850 1022 837 728 854 723 846 745 ... #&gt; $ arr_delay : num 11 20 33 -18 -25 12 19 -14 -8 8 ... #&gt; $ carrier : chr &quot;UA&quot; &quot;UA&quot; &quot;AA&quot; &quot;B6&quot; ... #&gt; $ flight : int 1545 1714 1141 725 461 1696 507 5708 79 301 ... #&gt; $ tailnum : chr &quot;N14228&quot; &quot;N24211&quot; &quot;N619AA&quot; &quot;N804JB&quot; ... #&gt; $ origin : chr &quot;EWR&quot; &quot;LGA&quot; &quot;JFK&quot; &quot;JFK&quot; ... #&gt; $ dest : chr &quot;IAH&quot; &quot;IAH&quot; &quot;MIA&quot; &quot;BQN&quot; ... #&gt; $ air_time : num 227 227 160 183 116 150 158 53 140 138 ... #&gt; $ distance : num 1400 1416 1089 1576 762 ... #&gt; $ hour : num 5 5 5 5 6 5 6 6 6 6 ... #&gt; $ minute : num 15 29 40 45 0 58 0 0 0 0 ... #&gt; $ time_hour : POSIXct, format: &quot;2013-01-01 05:00:00&quot; &quot;2013-01-01 05:00:00&quot; ... # add column of unique row ids flights$row_id = 1:nrow(flights) # create sqlite database in temporary file path = tempfile(&quot;flights&quot;, fileext = &quot;.sqlite&quot;) con = DBI::dbConnect(RSQLite::SQLite(), path) tbl = DBI::dbWriteTable(con, &quot;flights&quot;, as.data.frame(flights)) DBI::dbDisconnect(con) 5.3.2 Preprocessing with dplyr With the SQLite database in path, we now re-establish a connection and switch to dplyr/dbplyr for some essential pre # establish connection con = DBI::dbConnect(RSQLite::SQLite(), path) # select the &quot;flights&quot; table, enter dplyr library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:data.table&#39;: #&gt; #&gt; between, first, last #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union library(dbplyr) #&gt; #&gt; Attaching package: &#39;dbplyr&#39; #&gt; The following objects are masked from &#39;package:dplyr&#39;: #&gt; #&gt; ident, sql tbl = tbl(con, &quot;flights&quot;) First, we select a subset of columns to work on: keep = c(&quot;row_id&quot;, &quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;hour&quot;, &quot;minute&quot;, &quot;dep_time&quot;, &quot;arr_time&quot;, &quot;carrier&quot;, &quot;flight&quot;, &quot;air_time&quot;, &quot;distance&quot;, &quot;arr_delay&quot;) tbl = select(tbl, keep) Additionally, we remove those observations where the arrival delay (arr_delay) has a missing value: tbl = filter(tbl, !is.na(arr_delay)) And to keep the runtimes reasonable for this toy example, we filter the data to only use every second row: tbl = filter(tbl, row_id %% 2 == 0) Finally, we merge the factor levels of feature carrier so that infrequent carriers are replaced by level “other”: tbl = mutate(tbl, carrier = case_when( carrier %in% c(&quot;OO&quot;, &quot;HA&quot;, &quot;YV&quot;, &quot;F9&quot;, &quot;AS&quot;, &quot;FL&quot;, &quot;VX&quot;, &quot;WN&quot;) ~ &quot;other&quot;, TRUE ~ carrier) ) 5.3.3 DataBackendDplyr The processed table is now used to create a mlr3db::DataBackendDplyr from mlr3db: library(mlr3db) b = as_data_backend(tbl, primary_key = &quot;row_id&quot;) We can now use the interface of mlr3::DataBackend to query some basic information of the data: b$nrow #&gt; [1] 163707 b$ncol #&gt; [1] 13 b$head() #&gt; row_id year month day hour minute dep_time arr_time carrier flight air_time #&gt; 1: 2 2013 1 1 5 29 533 850 UA 1714 227 #&gt; 2: 4 2013 1 1 5 45 544 1004 B6 725 183 #&gt; 3: 6 2013 1 1 5 58 554 740 UA 1696 150 #&gt; 4: 8 2013 1 1 6 0 557 709 EV 5708 53 #&gt; 5: 10 2013 1 1 6 0 558 753 AA 301 138 #&gt; 6: 12 2013 1 1 6 0 558 853 B6 71 158 #&gt; distance arr_delay #&gt; 1: 1416 20 #&gt; 2: 1576 -18 #&gt; 3: 719 12 #&gt; 4: 229 -14 #&gt; 5: 733 8 #&gt; 6: 1005 -3 Note that the DataBackendDplyr does not know about any rows or columns we have filtered out with dplyr before, it just operates on the view we provided. 5.3.4 Model fitting We create the following mlr3 objects: A regression task, based on the previously created mlr3db::DataBackendDplyr. A regression learner (regr.rpart). A resampling strategy: 3 times repeated subsampling using 2% of the observations for training (“subsampling”) Measures “mse”, “time_predict” and “time_predict” task = TaskRegr$new(&quot;flights_sqlite&quot;, b, target = &quot;arr_delay&quot;) learner = mlr_learners$get(&quot;regr.rpart&quot;) measures = mlr_measures$mget(c(&quot;regr.mse&quot;, &quot;time_train&quot;, &quot;time_predict&quot;)) resampling = mlr_resamplings$get(&quot;subsampling&quot;) resampling$param_set$values = list(repeats = 3, ratio = 0.02) We pass all these objects to resample() to perform a simple resampling with three iterations. In each iteration, only the required subset of the data is queried from the SQLite data base and passed to rpart::rpart(): rr = resample(task, learner, resampling) print(rr) #&gt; &lt;ResampleResult&gt; of 3 iterations #&gt; Task: flights_sqlite #&gt; Learner: regr.rpart rr$aggregate(measures) #&gt; regr.mse time_train time_predict #&gt; 1164.045 0.176 1.313 5.3.5 Cleanup Finally, we remove the tbl object and close the connection. rm(tbl) DBI::dbDisconnect(con) "],
["paradox.html", "5.4 Parameters (using paradox)", " 5.4 Parameters (using paradox) The paradox package offers a language for the description of parameter spaces, as well as tools for useful operations on these parameter spaces. A parameter space is often useful when describing a set of sensible input values for an R function, the set of possible values that slots of a configuration object can take, or the search space of an optimization process. The tools provided by paradox therefore relate to - Parameter checking: Verifying that a set of parameters satisfies the conditions of a parameter space and - Parameter sampling: Generating parameter values that lie in the parameter space for systematic exploration of program behavior depending on these parameters. paradox is, by its nature, an auxiliary package that derives its usefulness from other packages that make use of it. It is heavily utilized in other mlr-org packages such as mlr3, mlr3pipelines, and mlr3tuning. 5.4.1 Reference Based Objects paradox is the spiritual successor to the ParamHelpers package and was written from scratch using the R6 class system. The most important consequence of this is that all objects created in paradox are “reference-based”, unlike most other objects in R. When a change is made to a ParamSet object, for example by adding a parameter using the $add() function, all variables that point to this ParamSet will contain the changed object. To create an independent copy of a ParamSet, the $clone() method needs to be used: library(&quot;paradox&quot;) ps = ParamSet$new() ps2 = ps ps3 = ps$clone(deep = TRUE) print(ps) # the same for ps2 and ps3 #&gt; ParamSet: #&gt; Empty. ps$add(ParamLgl$new(&quot;a&quot;)) print(ps) # ps was changed #&gt; ParamSet: #&gt; id class lower upper levels default value #&gt; 1: a ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; print(ps2) # contains the same reference as ps #&gt; ParamSet: #&gt; id class lower upper levels default value #&gt; 1: a ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; print(ps3) # is a &quot;clone&quot; of the old (empty) ps #&gt; ParamSet: #&gt; Empty. 5.4.2 Defining a Parameter Space 5.4.2.1 Single Parameters The basic building block for describing parameter spaces is the Param class. It represents a single parameter, which usually can take a single atomic value. Consider, for example, trying to configure the rpart package’s rpart.control object. It has various components (minsplit, cp, …) that all take a single value, and that would all be represented by a different instance of a Param object. The Param class has various subclasses that represent different value types: ParamInt: Integer numbers ParamDbl: Real numbers ParamFct: String values from a set of possible values, similar to R factors ParamLgl: Truth values (TRUE / FALSE), as logicals in R ParamUty: Parameter that can take any value A particular instance of a parameter is created by calling the attached $new() function: library(&quot;paradox&quot;) parA = ParamLgl$new(id = &quot;A&quot;) parB = ParamInt$new(id = &quot;B&quot;, lower = 0, upper = 10, tags = c(&quot;tag1&quot;, &quot;tag2&quot;)) parC = ParamDbl$new(id = &quot;C&quot;, lower = 0, upper = 4, special_vals = list(NULL)) parD = ParamFct$new(id = &quot;D&quot;, levels = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;), default = &quot;y&quot;) parE = ParamUty$new(id = &quot;E&quot;, custom_check = function(x) checkmate::checkFunction(x)) Every parameter must have an id—its name within the parameter set—, and may also have a default value (default), a list of values that are accepted even if they do not conform to the type (special_vals). tags that can be used to organize parameters. The numeric (Int and Dbl) parameters furthermore allow for specification of a lower and upper bound, while the Fct parameter must be given a vector of levels that define the possible states its parameter can take. The Uty parameter can also have a custom_check function that must return TRUE when a value is acceptable and may return a character(1) error description otherwise. The example above defines parE as a parameter that only accepts functions. All values which are given to the constructor are then accessible from the object for inspection using $. Although all these values can be changed for a parameter after construction, this can be a bad idea and should be avoided when possible. Instead, a new parameter should be constructed. Besides the possible values that can be given to a constructor, there are also the $class, $nlevels, $is_bounded, $has_default, $storage_type, $is_number and $is_categ slots that give information about a parameter. A list of all slots can be found in ?Param. parB$lower #&gt; [1] 0 parA$levels #&gt; [1] TRUE FALSE parE$class #&gt; [1] &quot;ParamUty&quot; It is also possible to get all information of a Param as data.table by calling as.data.table. as.data.table(parA) #&gt; id class lower upper levels nlevels is_bounded special_vals default storage_type tags #&gt; 1: A ParamLgl NA NA TRUE,FALSE 2 TRUE &lt;list&gt; &lt;NoDefault&gt; logical 5.4.2.1.1 Type / Range Checking A Param object offers the possibility to check whether a value satisfies its condition, i.e. is of the right type, and also falls within the range of allowed values, using the $test(), $check(), and $assert() functions. test() should be used within conditional checks and returns TRUE or FALSE, while check() returns an error description when a value does not conform to the parameter (and thus plays well with the checkmate::assert() function). assert() will throw an error whenever a value does not fit. parA$test(FALSE) #&gt; [1] TRUE parA$test(&quot;FALSE&quot;) #&gt; [1] FALSE parA$check(&quot;FALSE&quot;) #&gt; [1] &quot;Must be of type &#39;logical flag&#39;, not &#39;character&#39;&quot; Instead of testing single parameters, it is often more convenient to check a whole set of parameters using a ParamSet. 5.4.2.2 Parameter Sets The ordered collection of parameters is handled in a ParamSet1. It is initialized using the $new() function and optionally takes a list of Params as argument. Parameters can also be added to the constructed ParamSet using the $add() function. It is even possible to add whole ParamSets to other ParamSets. ps = ParamSet$new(list(parA, parB)) ps$add(parC) ps$add(ParamSet$new(list(parD, parE))) print(ps) #&gt; ParamSet: #&gt; id class lower upper levels default value #&gt; 1: A ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; #&gt; 2: B ParamInt 0 10 &lt;NoDefault&gt; #&gt; 3: C ParamDbl 0 4 &lt;NoDefault&gt; #&gt; 4: D ParamFct NA NA x,y,z y #&gt; 5: E ParamUty NA NA &lt;NoDefault&gt; The individual parameters can be accessed through the $params slot. It is also possible to get information about all parameters in a vectorized fashion using mostly the same slots as for individual Params (i.e. $class, $levels etc.), see ?ParamSet for details. It is possible to reduce ParamSets using the $subset method. Be aware that it modifies a ParamSet in-place, so a “clone” must be created first if the original ParamSet should not be modified. psSmall = ps$clone() psSmall$subset(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) print(psSmall) #&gt; ParamSet: #&gt; id class lower upper levels default value #&gt; 1: A ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; #&gt; 2: B ParamInt 0 10 &lt;NoDefault&gt; #&gt; 3: C ParamDbl 0 4 &lt;NoDefault&gt; Just as for Params, and much more useful, it is possible to get the ParamSet as a data.table using as.data.table(). This makes it easy to subset parameters on certain conditions and aggregate information about them, using the variety of methods provided by data.table. as.data.table(ps) #&gt; id class lower upper levels nlevels is_bounded special_vals default storage_type tags #&gt; 1: A ParamLgl NA NA TRUE,FALSE 2 TRUE &lt;list&gt; &lt;NoDefault&gt; logical #&gt; 2: B ParamInt 0 10 11 TRUE &lt;list&gt; &lt;NoDefault&gt; integer tag1,tag2 #&gt; 3: C ParamDbl 0 4 Inf TRUE &lt;list&gt; &lt;NoDefault&gt; numeric #&gt; 4: D ParamFct NA NA x,y,z 3 TRUE &lt;list&gt; y character #&gt; 5: E ParamUty NA NA Inf FALSE &lt;list&gt; &lt;NoDefault&gt; list 5.4.2.2.1 Type / Range Checking Similar to individual Params, the ParamSet provides $test(), $check() and $assert() functions that allow for type and range checking of parameters. Their argument must be a named list with values that are checked against the respective parameters. It is possible to check only a subset of parameters. ps$check(list(A = TRUE, B = 0, E = identity)) #&gt; [1] TRUE ps$check(list(A = 1)) #&gt; [1] &quot;A: Must be of type &#39;logical flag&#39;, not &#39;double&#39;&quot; ps$check(list(Z = 1)) #&gt; [1] &quot;Parameter &#39;Z&#39; not available. Did you mean &#39;A&#39; / &#39;B&#39; / &#39;C&#39;?&quot; 5.4.2.2.2 Values in a ParamSet Although a ParamSet fundamentally represents a value space, it also has a slot $values that can contain a point within that space. This is useful because many things that define a parameter space need similar operations (like parameter checking) that can be simplified. The $values slot contains a named list that is always checked against parameter constraints. When trying to set parameter values, e.g. for mlr3 Learners, it is the $values slot of its $param_set that needs to be used. ps$values = list(A = TRUE, B = 0) ps$values$B = 1 print(ps$values) #&gt; $A #&gt; [1] TRUE #&gt; #&gt; $B #&gt; [1] 1 The parameter constraints are automatically checked: ps$values$B = 100 #&gt; Error in (function (xs) : Assertion on &#39;xs&#39; failed: B: Element 1 is not &lt;= 10. 5.4.2.2.3 Dependencies It is often the case that certain parameters are irrelevant or should not be given depending on values of other parameters. An example would be a parameter that switches a certain algorithm feature (for example regularization) on or off, combined with another parameter that controls the behavior of that feature (e.g. a regularization parameter). The second parameter would be said to depend on the first parameter having the value TRUE. A dependency can be added using the $add_dep method, which takes both the ids of the “depender” and “dependee” parameters as well as a Condition object. The Condition object represents the check to be performed on the “dependee”. Currently it can be created using CondEqual$new() and CondAnyOf$new(). Multiple dependencies can be added, and parameters that depend on others can again be depended on, as long as no cyclic dependencies are introduced. The consequence of dependencies are twofold: For one, the $check(), $test() and $assert() tests will not accept the presence of a parameter if its dependency is not met. Furthermore, when sampling or creating grid designs from a ParamSet, the dependencies will be respected (see Parameter Sampling, in particular Hierarchical Sampler). The following example makes parameter D depend on parameter A being FALSE, and parameter B depend on parameter D being one of \"x\" or \"y\". This introduces an implicit dependency of B on A being FALSE as well, because D does not take any value if A is TRUE. ps$add_dep(&quot;D&quot;, &quot;A&quot;, CondEqual$new(FALSE)) ps$add_dep(&quot;B&quot;, &quot;D&quot;, CondAnyOf$new(c(&quot;x&quot;, &quot;y&quot;))) ps$check(list(A = FALSE, D = &quot;x&quot;, B = 1)) # OK: all dependencies met #&gt; [1] TRUE ps$check(list(A = FALSE, D = &quot;z&quot;, B = 1)) # B&#39;s dependency is not met #&gt; [1] &quot;Condition for &#39;B&#39; not ok: D anyof x, y; instead: D=z&quot; ps$check(list(A = FALSE, B = 1)) # B&#39;s dependency is not met #&gt; [1] &quot;Condition for &#39;B&#39; not ok: D anyof x, y; instead: D=&lt;not-there&gt;&quot; ps$check(list(A = FALSE, D = &quot;z&quot;)) # OK: B is absent #&gt; [1] TRUE ps$check(list(A = TRUE)) # OK: neither B nor D present #&gt; [1] TRUE ps$check(list(A = TRUE, D = &quot;x&quot;, B = 1)) # D&#39;s dependency is not met #&gt; [1] &quot;Condition for &#39;D&#39; not ok: A equal FALSE; instead: A=TRUE&quot; ps$check(list(A = TRUE, B = 1)) # B&#39;s dependency is not met #&gt; [1] &quot;Condition for &#39;B&#39; not ok: D anyof x, y; instead: D=&lt;not-there&gt;&quot; Internally, the dependencies are represented as a data.table, which can be accessed listed in the $deps slot. This data.table can even be mutated, for example to remove dependencies. There are no sanity checks done when the $deps slot is changed this way, so be careful. ps$deps #&gt; id on cond #&gt; 1: D A &lt;CondEqual&gt; #&gt; 2: B D &lt;CondAnyOf&gt; 5.4.2.3 Vector Parameters Unlike in the old ParamHelpers package, there are no more vectorial parameters in paradox. Instead, it is now possible to create multiple copies of a single parameter using the $rep function. This creates a ParamSet consisting of multiple copies of the parameter, which can then (optionally) be added to another ParamSet. ps2d = ParamDbl$new(&quot;x&quot;, lower = 0, upper = 1)$rep(2) print(ps2d) #&gt; ParamSet: #&gt; id class lower upper levels default value #&gt; 1: x_rep_1 ParamDbl 0 1 &lt;NoDefault&gt; #&gt; 2: x_rep_2 ParamDbl 0 1 &lt;NoDefault&gt; ps$add(ps2d) print(ps) #&gt; ParamSet: #&gt; id class lower upper levels default parents value #&gt; 1: A ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; TRUE #&gt; 2: B ParamInt 0 10 &lt;NoDefault&gt; D 1 #&gt; 3: C ParamDbl 0 4 &lt;NoDefault&gt; #&gt; 4: D ParamFct NA NA x,y,z y A #&gt; 5: E ParamUty NA NA &lt;NoDefault&gt; #&gt; 6: x_rep_1 ParamDbl 0 1 &lt;NoDefault&gt; #&gt; 7: x_rep_2 ParamDbl 0 1 &lt;NoDefault&gt; It is also possible to use a ParamUty to accept vectorial parameters, which also works for parameters of variable length. A ParamSet containing a ParamUty can be used for parameter checking, but not for sampling. To sample values for a method that needs a vectorial parameter, it is advised to use a parameter transformation function that creates a vector from atomic values. Assembling a vector from repeated parameters is aided by the parameter’s $tags: Parameters that were generated by the $rep() command automatically get tagged as belonging to a group of repeated parameters: ps$tags #&gt; $A #&gt; character(0) #&gt; #&gt; $B #&gt; [1] &quot;tag1&quot; &quot;tag2&quot; #&gt; #&gt; $C #&gt; character(0) #&gt; #&gt; $D #&gt; character(0) #&gt; #&gt; $E #&gt; character(0) #&gt; #&gt; $x_rep_1 #&gt; [1] &quot;x_rep&quot; #&gt; #&gt; $x_rep_2 #&gt; [1] &quot;x_rep&quot; 5.4.3 Parameter Sampling It is often useful to have a list of possible parameter values that can be systematically iterated through, for example to find parameter values for which an algorithm performs particularly well (tuning). paradox offers a variety of functions that allow creating evenly-spaced parameter values in a “grid” design as well as random sampling. In the latter case, it is possible to influence the sampling distribution in more or less fine detail. A point to always keep in mind while sampling is that only numerical and factorial parameters that are bounded can be sampled from, i.e. not ParamUty. Furthermore, for most samplers ParamInt and ParamDbl must have finite lower and upper bounds. 5.4.3.1 Parameter Designs Functions that sample the parameter space fundamentally return an object of the Design class. These objects contain the sampled data as a data.table under the $data slot, and also offer conversion to a list of parameter-values using the $transpose() function. 5.4.3.2 Grid Design The generate_design_grid() function is used to create grid designs that contain all combinations of parameter values: All possible values for ParamLgl and ParamFct, and values with a given resolution for ParamInt and ParamDbl. The resolution can be given for all numeric parameters, or for specific named parameters through the param_resolutions parameter. design = generate_design_grid(psSmall, 2) print(design) #&gt; &lt;Design&gt; with 8 rows: #&gt; A B C #&gt; 1: TRUE 0 0 #&gt; 2: TRUE 0 4 #&gt; 3: TRUE 10 0 #&gt; 4: TRUE 10 4 #&gt; 5: FALSE 0 0 #&gt; 6: FALSE 0 4 #&gt; 7: FALSE 10 0 #&gt; 8: FALSE 10 4 generate_design_grid(psSmall, param_resolutions = c(B = 1, C = 2)) #&gt; &lt;Design&gt; with 4 rows: #&gt; B C A #&gt; 1: 0 0 TRUE #&gt; 2: 0 0 FALSE #&gt; 3: 0 4 TRUE #&gt; 4: 0 4 FALSE 5.4.3.3 Random Sampling paradox offers different methods for random sampling, which vary in the degree to which they can be configured. The easiest way to get a uniformly random sample of parameters is generate_design_random. It is also possible to create “latin hypercube” sampled parameter values using generate_design_lhs, which utilizes the lhs package. LHS-sampling creates low-discrepancy sampled values that cover the parameter space more evenly than purely random values. pvrand = generate_design_random(ps2d, 500) pvlhs = generate_design_lhs(ps2d, 500) 5.4.3.4 Generalized Sampling: The Sampler Class It may sometimes be desirable to configure parameter sampling in more detail. paradox uses the Sampler abstract base class for sampling, which has many different subclasses that can be parameterized and combined to control the sampling process. It is even possible to create further subclasses of the Sampler class (or of any of its subclasses) for even more possibilities. Every Sampler object has a sample() function, which takes one argument, the number of instances to sample, and returns a Design object. 5.4.3.4.1 1D-Samplers There is a variety of samplers that sample values for a single parameter. These are Sampler1DUnif (uniform sampling), Sampler1DCateg (sampling for categorical parameters), Sampler1DNormal (normally distributed sampling, truncated at parameter bounds), and Sampler1DRfun (arbitrary 1D sampling, given a random-function). These are initialized with a single Param, and can then be used to sample values. sampA = Sampler1DCateg$new(parA) sampA$sample(5) #&gt; &lt;Design&gt; with 5 rows: #&gt; A #&gt; 1: FALSE #&gt; 2: TRUE #&gt; 3: TRUE #&gt; 4: FALSE #&gt; 5: TRUE 5.4.3.4.2 Hierarchical Sampler The SamplerHierarchical sampler is an auxiliary sampler that combines many 1D-Samplers to get a combined distribution. Its name “hierarchical” implies that it is able to respect parameter dependencies: Parameters only get sampled when their dependencies are met. The following example shows how this works: The Int parameter B depends on the Lgl parameter A being TRUE. A is sampled to be TRUE in about half the cases, in which case B takes a value between 0 and 10. In the cases where A is FALSE, B is set to NA. psSmall$add_dep(&quot;B&quot;, &quot;A&quot;, CondEqual$new(TRUE)) sampH = SamplerHierarchical$new(psSmall, list(Sampler1DCateg$new(parA), Sampler1DUnif$new(parB), Sampler1DUnif$new(parC)) ) sampled = sampH$sample(1000) table(sampled$data[, c(&quot;A&quot;, &quot;B&quot;)], useNA = &quot;ifany&quot;) #&gt; B #&gt; A 0 1 2 3 4 5 6 7 8 9 10 &lt;NA&gt; #&gt; FALSE 0 0 0 0 0 0 0 0 0 0 0 520 #&gt; TRUE 37 47 50 39 46 56 45 46 34 45 35 0 5.4.3.4.3 Joint Sampler Another way of combining samplers is the SamplerJointIndep. It also makes it possible to combine Samplers that are not 1D. However, it currently can not handle ParamSets with dependencies. sampJ = SamplerJointIndep$new( list(Sampler1DUnif$new(ParamDbl$new(&quot;x&quot;, 0, 1)), Sampler1DUnif$new(ParamDbl$new(&quot;y&quot;, 0, 1))) ) sampJ$sample(5) #&gt; &lt;Design&gt; with 5 rows: #&gt; x y #&gt; 1: 0.38298 0.9322 #&gt; 2: 0.18512 0.8948 #&gt; 3: 0.75034 0.8110 #&gt; 4: 0.24598 0.2733 #&gt; 5: 0.09854 0.9907 5.4.3.4.4 SamplerUnif The Sampler used in generate_design_random is the SamplerUnif sampler, which corresponds to a HierarchicalSampler of Sampler1DUnif for all parameters. 5.4.4 Parameter Transformation While the different Samplers allow for a wide specification of parameter distributions, there are cases where the simplest way of getting a desired distribution is to sample parameters from a simple distribution (such as the uniform distribution) and then transform them. This can be done by assigning a function to the $trafo slot of a ParamSet. The $trafo function is called with two parameters: The list of parameter values to be transformed as x, and the ParamSet itself as param_set; it must return a list of transformed parameter values. The transformation is performed when calling the $transpose function of the Design object returned by a Sampler with the trafo ParamSet to TRUE (the default). The following, for example, creates a parameter that is exponentially distributed: psexp = ParamSet$new(list(ParamDbl$new(&quot;par&quot;, 0, 1))) psexp$trafo = function(x, param_set) { x$par = -log(x$par) x } design = generate_design_random(psexp, 2) print(design) #&gt; &lt;Design&gt; with 2 rows: #&gt; par #&gt; 1: 0.4980 #&gt; 2: 0.2379 design$transpose() # trafo is TRUE #&gt; [[1]] #&gt; [[1]]$par #&gt; [1] 0.6971 #&gt; #&gt; #&gt; [[2]] #&gt; [[2]]$par #&gt; [1] 1.436 Compare this to $transpose() without transformation: design$transpose(trafo = FALSE) #&gt; [[1]] #&gt; [[1]]$par #&gt; [1] 0.498 #&gt; #&gt; #&gt; [[2]] #&gt; [[2]]$par #&gt; [1] 0.2379 5.4.4.1 Transformation between Types Usually the design created with one ParamSet is then used to configure other objects that themselves have a ParamSet which defines the values they take. The ParamSets which can be used for random sampling, however, are restricted in some ways: They must have finite bounds, and they may not contain “untyped” (ParamUty) parameters. $trafo provides the glue for these situations. There is relatively little constraint on the trafo function’s return value, so it is possible to return values that have different bounds or even types than the original ParamSet. It is even possible to remove some parameters and add new ones. Suppose, for example, that a certain method requires a function as a parameter, Let’s say a function that summarizes its data in a certain way. The user can pass functions like median() or mean(), but could also pass quantiles or something completely different. This method would probably use the following ParamSet: methodPS = ParamSet$new( list( ParamUty$new(&quot;fun&quot;, custom_check = function(x) checkmate::checkFunction(x, nargs = 1)) ) ) print(methodPS) #&gt; ParamSet: #&gt; id class lower upper levels default value #&gt; 1: fun ParamUty NA NA &lt;NoDefault&gt; If one wanted to sample this method, using one of four functions, a way to do this would be samplingPS = ParamSet$new( list( ParamFct$new(&quot;fun&quot;, c(&quot;mean&quot;, &quot;median&quot;, &quot;min&quot;, &quot;max&quot;)) ) ) samplingPS$trafo = function(x, param_set) { # x$fun is a `character(1)`, # in particular one of &#39;mean&#39;, &#39;median&#39;, &#39;min&#39;, &#39;max&#39;. # We want to turn it into a function! x$fun = get(x$fun, mode = &quot;function&quot;) x } design = generate_design_random(samplingPS, 2) print(design) #&gt; &lt;Design&gt; with 2 rows: #&gt; fun #&gt; 1: mean #&gt; 2: min Note that the Design only contains the column “fun” as a character column. To get a single value as a function, the $transpose function is used. xvals = design$transpose() print(xvals[[1]]) #&gt; $fun #&gt; function (x, ...) #&gt; UseMethod(&quot;mean&quot;) #&gt; &lt;bytecode: 0x2c11ff8&gt; #&gt; &lt;environment: namespace:base&gt; We can now check that it fits the requirements set by methodPS, and that fun it is in fact a function: methodPS$check(xvals[[1]]) #&gt; [1] TRUE xvals[[1]]$fun(1:10) #&gt; [1] 5.5 Imagine now that a different kind of parametrization of the function is desired: The user wants to give a function that selects a certain quantile, where the quantile is set by a parameter. In that case the $transpose function could generate a function in a different way. For interpretability, the parameter is called “quantile” before transformation, and the “fun” parameter is generated on the fly. samplingPS2 = ParamSet$new( list( ParamDbl$new(&quot;quantile&quot;, 0, 1) ) ) samplingPS2$trafo = function(x, param_set) { # x$quantile is a `numeric(1)` between 0 and 1. # We want to turn it into a function! list(fun = function(input) quantile(input, x$quantile)) } design = generate_design_random(samplingPS2, 2) print(design) #&gt; &lt;Design&gt; with 2 rows: #&gt; quantile #&gt; 1: 0.7622 #&gt; 2: 0.1686 The Design now contains the column “quantile” that will be used by the $transpose function to create the fun parameter. We also check that it fits the requirement set by methodPS, and that it is a function. xvals = design$transpose() print(xvals[[1]]) #&gt; $fun #&gt; function(input) quantile(input, x$quantile) #&gt; &lt;environment: 0x1b230118&gt; methodPS$check(xvals[[1]]) #&gt; [1] TRUE xvals[[1]]$fun(1:10) #&gt; 76.22% #&gt; 7.86 Although the name is suggestive of a “Set”-valued Param, this is unrelated to the other objects that follow the ParamXxx naming scheme.↩ "],
["extending.html", "5.5 Extending mlr3", " 5.5 Extending mlr3 5.5.1 Learners Here, we show how to create a custom LearnerClassif step-by-step. Preferably, you start by copying over code from an existing Learner, e.g. from the \"classif.rpart learner on GitHub. Alternatively, here is a template for a new classification learner: LearnerClassifYourLearner = R6::R6Class(&quot;LearnerClassifYourLearner&quot;, inherit = LearnerClassif, public = list( initialize = function(id = &quot;classif.yourlearner&quot;) { super$initialize( id = id, param_set = ParamSet$new(), param_vals = list() predict_types = , feature_types = , properties = , packages = , ) }, train = function(task) { }, predict = function(task) { } ) ) In the first line of the template, we create a new R6 class with class \"LearnerClassifYourLearner\". The next line determines the parent class: As we want to create a classification learner, we obviously want to inherit from LearnerClassif. A learner consists of three parts: Meta information about the learners A train_internal() function which takes a (filtered) TaskClassif and returns a model A predict_internal() function which operates on the model in self$model (stored during $train()) and a (differently subsetted) TaskClassif to return a named list of predictions. 5.5.1.1 Meta-information In the constructor function initialize() the constructor of the super class LearnerClassif is called with meta information about the leaner we want to construct. This includes: id: The id of the new learner. param_set: A set of hyperparameters and their description, provided as paradox::ParamSet. param_vals: Default hyperparameter settings as named list. predict_types: Set of predict types the learner is capable of. For classification, this must be a subset of “response”, “c(”response“,”prob“)”. See mlr_reflection$learner_predict_types for possible predict types of other tasks. feature_types: Set of feature types the learner can handle. See mlr_reflections$task_feature_types for feature types supported by mlr3. properties: Set of properties of the learner. Possible properties include: \"twoclass\": The learner works on binary classification problems. \"multiclass\": The learner works on multi-class classification problems. \"missings\": The learner can natively handle missing values. \"weights\": The learner can work on tasks which have observation weights / case weights. \"parallel\": The learner can be parallelized, e.g. via threading. \"importance\": The learner supports extracting importance values for features. If this property is set, you must also implement a public method importance() to retrieve the importance values from the model. \"selected features\": The learner supports extracting the features which where used. If this property is set, you must also implement a public method selected_features() to retrieve the set of used features from the model. Set of required packages to run the learner. For a simplified rpart::rpart(), the initialization could look like this: initialize = function(id = &quot;classif.rpart&quot;) { super$initialize( id = id, packages = &quot;rpart&quot;, feature_types = c(&quot;logical&quot;, &quot;integer&quot;, &quot;numeric&quot;, &quot;factor&quot;), predict_types = c(&quot;response&quot;, &quot;prob&quot;), param_set = ParamSet$new( params = list( ParamDbl$new(id = &quot;cp&quot;, default = 0.01, lower = 0, upper = 1, tags = &quot;train&quot;), ParamInt$new(id = &quot;xval&quot;, default = 0L, lower = 0L, tags = &quot;train&quot;) ) ), param_vals = list(xval = 0L), properties = c(&quot;twoclass&quot;, &quot;multiclass&quot;, &quot;weights&quot;, &quot;missings&quot;) ) } We only have specified a small subset of the available hyperparameters: The complexity \"cp\" is numeric, its feasible range is [0,1], it defaults to 0.01 and the parameter is used during \"train\". The complexity \"xval\" is integer, its lower bound 0, its default is 0 and the parameter is also used during \"train\". Note that we have changed the default here from 10 to 0 to save some computation time. This is not done by setting a different default in ParamInt$new(), but instead by setting the value implicitly via param_vals. 5.5.1.2 Train function We continue the to adept the template for a rpart::rpart() learner, and now tackle the train_internal() function. The train function takes a Task as input and must return an arbitrary model. First, we write something down that works completely without mlr3: data = iris model = rpart::rpart(Species ~ ., data = iris, xval = 0) In the next step, we replace the data frame data with a Task: task = mlr_tasks$get(&quot;iris&quot;) model = rpart::rpart(Species ~ ., data = task$data(), xval = 0) The target variable \"Species\" is still hard-coded and specific to the task. This is unnecessary, as the information about the target variable is stored in the task: task$target_names #&gt; [1] &quot;Species&quot; task$formula() #&gt; Species ~ . #&gt; NULL We can adapt our code accordingly: rpart::rpart(task$formula(), data = task$data(), xval = 0) #&gt; n= 150 #&gt; #&gt; node), split, n, loss, yval, (yprob) #&gt; * denotes terminal node #&gt; #&gt; 1) root 150 100 setosa (0.33333 0.33333 0.33333) #&gt; 2) Petal.Length&lt; 2.45 50 0 setosa (1.00000 0.00000 0.00000) * #&gt; 3) Petal.Length&gt;=2.45 100 50 versicolor (0.00000 0.50000 0.50000) #&gt; 6) Petal.Width&lt; 1.75 54 5 versicolor (0.00000 0.90741 0.09259) * #&gt; 7) Petal.Width&gt;=1.75 46 1 virginica (0.00000 0.02174 0.97826) * The last thing missing is the handling of hyperparameters. Instead of the hard-coded xval, we query the hyperparameter settings from the Learner itself. To illustrate this, we quickly construct the tree learner from the mlr3 package, and use the method get_value() from the ParamSet to retrieve all set hyperparameters with tag \"train\". self = mlr_learners$get(&quot;classif.rpart&quot;) self$param_set$get_values(tags = &quot;train&quot;) #&gt; $xval #&gt; [1] 0 To pass all hyperparameters down to the model fitting function, we recommend to use either do.call or the function mlr3misc::invoke(). pars = self$param_set$get_values(tags = &quot;train&quot;) mlr3misc::invoke(rpart::rpart, task$formula(), data = task$data(), .args = pars) #&gt; n= 150 #&gt; #&gt; node), split, n, loss, yval, (yprob) #&gt; * denotes terminal node #&gt; #&gt; 1) root 150 100 setosa (0.33333 0.33333 0.33333) #&gt; 2) Petal.Length&lt; 2.45 50 0 setosa (1.00000 0.00000 0.00000) * #&gt; 3) Petal.Length&gt;=2.45 100 50 versicolor (0.00000 0.50000 0.50000) #&gt; 6) Petal.Width&lt; 1.75 54 5 versicolor (0.00000 0.90741 0.09259) * #&gt; 7) Petal.Width&gt;=1.75 46 1 virginica (0.00000 0.02174 0.97826) * In the final learner, self will of course reference the learner itself. In the last step, we wrap everything in a function. train_internal = function(task) { pars = self$param_set$get_values(tags = &quot;train&quot;) mlr3misc::invoke(rpart::rpart, task$formula(), data = task$data(), .args = pars) } 5.5.1.3 Predict function The internal predict function predict_internal also operates on a Task as well as on the model stored during train() in self$model. The return value is a named list which will be automatically casted to a Prediction object with the learner method $new_prediction(). We proceed analogously to the section on the train function: We start with a version without any mlr3 objects and continue to replace objects until we have reached the desired interface: # inputs: task = mlr_tasks$get(&quot;iris&quot;) self = list(model = rpart::rpart(task$formula(), data = task$data())) data = iris response = predict(self$model, newdata = data, type = &quot;class&quot;) prob = predict(self$model, newdata = data, type = &quot;prob&quot;) The rpart::predict.rpart() function predicts class labels if argument type is set to to \"class\", and class probabilities if set to \"prob\". Next, we transition from data to a task again. Additionally, as we do not want to run the prediction twice, we differentiate what type of prediction is requested by querying the set predict type of the learner. The complete predict_internal function looks like this: predict_internal = function(task) { self$predict_type = &quot;response&quot; if (self$predict_type == &quot;response&quot;) { list(response = predict(self$model, newdata = task$data(), type = &quot;class&quot;)) } else { list(prob = predict(self$model, newdata = task$data(), type = &quot;prob&quot;)) } } Note that if the learner would need to handle hyperparameters during the predict step, we would proceed accordingly to the train() step and use self$params(\"predict\") in combination with mlr3misc::invoke(). 5.5.1.4 Final learner LearnerClassifYourRpart = R6::R6Class(&quot;LearnerClassifYourRpart&quot;, inherit = LearnerClassif, public = list( initialize = function(id = &quot;classif.rpart&quot;) { super$initialize( id = id, packages = &quot;rpart&quot;, feature_types = c(&quot;logical&quot;, &quot;integer&quot;, &quot;numeric&quot;, &quot;factor&quot;), predict_types = c(&quot;response&quot;, &quot;prob&quot;), param_set = paradox::ParamSet$new( params = list( paradox::ParamDbl$new(id = &quot;cp&quot;, default = 0.01, lower = 0, upper = 1, tags = &quot;train&quot;), paradox::ParamInt$new(id = &quot;xval&quot;, default = 0L, lower = 0L, tags = &quot;train&quot;) ) ), param_vals = list(xval = 0L), properties = c(&quot;twoclass&quot;, &quot;multiclass&quot;, &quot;weights&quot;, &quot;missings&quot;) ) }, train_internal = function(task) { pars = self$param_set$get_values(tag = &quot;train&quot;) mlr3misc::invoke(rpart::rpart, task$formula(), data = task$data(), .args = pars) }, predict_internal = function(task) { if (self$predict_type == &quot;response&quot;) { response = predict(self$model, newdata = task$data(), type = &quot;class&quot;) list(response = response) } else { prob = predict(self$model, newdata = task$data(), type = &quot;prob&quot;) list(prob = prob) } } ) ) lrn = LearnerClassifYourRpart$new() print(lrn) #&gt; &lt;LearnerClassifYourRpart:classif.rpart&gt; #&gt; Model: - #&gt; Parameters: xval=0 #&gt; Packages: rpart #&gt; Predict Type: response #&gt; Feature types: logical, integer, numeric, factor #&gt; Properties: missings, multiclass, twoclass, weights To run some basic tests: task = mlr_tasks$get(&quot;iris&quot;) lrn$train(task) p = lrn$predict(task) p$confusion #&gt; truth #&gt; response setosa versicolor virginica #&gt; setosa 50 0 0 #&gt; versicolor 0 49 5 #&gt; virginica 0 1 45 To run a bunch of automatic tests, you may source some auxiliary scripts from the unit tests of mlr3: helper = list.files(system.file(&quot;testthat&quot;, package = &quot;mlr3&quot;), pattern = &quot;^helper.*\\\\.[rR]&quot;, full.names = TRUE) ok = lapply(helper, source) stopifnot(run_autotest(lrn)) "],
["extending-pipes.html", "5.6 Extending mlr3pipelines", " 5.6 Extending mlr3pipelines This tutorial showcases how the mlr3pipelines package can be extended to include custom PipeOps. To run the following examples, we will need a Task; we are using the well-known “Iris” task: library(mlr3) task = mlr_tasks$get(&quot;iris&quot;) task$data() #&gt; Species Petal.Length Petal.Width Sepal.Length Sepal.Width #&gt; 1: setosa 1.4 0.2 5.1 3.5 #&gt; 2: setosa 1.4 0.2 4.9 3.0 #&gt; 3: setosa 1.3 0.2 4.7 3.2 #&gt; 4: setosa 1.5 0.2 4.6 3.1 #&gt; 5: setosa 1.4 0.2 5.0 3.6 #&gt; --- #&gt; 146: virginica 5.2 2.3 6.7 3.0 #&gt; 147: virginica 5.0 1.9 6.3 2.5 #&gt; 148: virginica 5.2 2.0 6.5 3.0 #&gt; 149: virginica 5.4 2.3 6.2 3.4 #&gt; 150: virginica 5.1 1.8 5.9 3.0 mlr3pipelines is fundamentally built around R6. When planning to create custom PipeOp objects, it can only help to familiarize yourself with it. In principle, all a PipeOp must do is inherit from the PipeOp R6 class and implement the train() and test() functions. There are, however, several auxiliary subclasses that can make the creation of certain operations much easier. 5.6.1 General Case Example: PipeOpCopy A very simple yet useful PipeOp is PipeOpCopy, which takes a single input and creates a variable number of output channels, all of which receive a copy of the input data. It is a simple example that showcases the important steps in defining a custom PipeOp. We will show a simplified version here, PipeOpCopyTwo, that creates exactly two copies of its input data. The following figure visualizes how our PipeOp is situated in the Pipeline and the significant in- and outputs. Pipeop Copy2 5.6.1.1 First Steps: Inheriting from PipeOp The first part of creating a custom PipeOp is inheriting from PipeOp. We make a mental note that we need to implement a train() and a predict() function, and that we probably want to have an initialize() as well: PipeOpCopyTwo = R6::R6Class(&quot;PipeOpCopyTwo&quot;, inherit = PipeOp, public = list( initialize = function(id = &quot;copy.two&quot;) { .... }, train = function(inputs) { .... }, predict = function(inputs) { .... } ) ) 5.6.1.2 Channel Definitions We need to tell the PipeOp the layout of its channels: How many there are, what their names are going to be, and what types are acceptable. This is done on initialization of the PipeOp (using a super$initialize call) by giving the input and output data.table objects. These must have three columns: a \"name\" column giving the names of input and output channels, and a \"train\" and \"predict\" column naming the class of objects we expect during training and prediction as input / output. A special value for these classes is \"*\", which indicates that any class will be accepted; our simple copy operator accepts any kind of input, so this will be useful. We have only one input, but two output channels. By convention, we name a single channel \"input\" or \"output\", and a group of channels [\"input1\", \"input2\", …], unless there is a reason to give specific different names. Therefore, our input data.table will have a single row &lt;\"input\", \"*\", \"*\"&gt;, and our output table will have two rows, &lt;\"output1\", \"*\", \"*\"&gt; and &lt;\"output2\", \"*\", \"*\"&gt;. All of this is given to the PipeOp creator. Our initialize() will thus look as follows: initialize = function(id = &quot;copy.two&quot;) { input = data.table::data.table(name = &quot;input&quot;, train = &quot;*&quot;, predict = &quot;*&quot;) # the following will create two rows and automatically fill the `train` # and `predict` cols with &quot;*&quot; output = data.table::data.table( name = c(&quot;output1&quot;, &quot;output2&quot;), train = &quot;*&quot;, predict = &quot;*&quot; ) super$initialize(id, input = input, output = output ) } 5.6.1.3 Train and Predict Both train() and predict() will receive a list as input and must give a list in return. According to our input and output definitions, we will always get a list with a single element as input, and will need to return a list with two elements. Because all we want to do is create two copies, we will just create the copies using c(inputs, inputs). Two things to consider: The train() function must always modify the self$state variable to something that is not NULL or NO_OP. This is because the $state slot is used as a signal that PipeOp has been trained on data, even if the state itself is not important to the PipeOp (as in our case). Therefore, our train() will set self$state = list(). It is not necessary to “clone” our input or make deep copies, because we don’t modify the data. However, if we were changing a reference-passed object, for example by changing data in a Task, we would have to make a deep copy first. This is because a PipeOp may never modify its input object by reference. Our train() and predict() functions are now: train = function(inputs) { self$state = list() c(inputs, inputs) }, predict = function(inputs) { c(inputs, inputs) } 5.6.1.4 Putting it Together The whole definition thus becomes PipeOpCopyTwo = R6::R6Class(&quot;PipeOpCopyTwo&quot;, inherit = PipeOp, public = list( initialize = function(id = &quot;copy.two&quot;) { super$initialize(id, input = data.table::data.table(name = &quot;input&quot;, train = &quot;*&quot;, predict = &quot;*&quot;), output = data.table::data.table(name = c(&quot;output1&quot;, &quot;output2&quot;), train = &quot;*&quot;, predict = &quot;*&quot;) ) }, train = function(inputs) { self$state = list() c(inputs, inputs) }, predict = function(inputs) { c(inputs, inputs) } ) ) We can create an instance of our PipeOp, put it in a graph, and see what happens when we train it on something: poct = PipeOpCopyTwo$new() gr = Graph$new() gr$add_pipeop(poct) print(gr) #&gt; Graph with 1 PipeOps: #&gt; ID State sccssors prdcssors #&gt; copy.two &lt;&lt;UNTRAINED&gt;&gt; result = gr$train(task) str(result) #&gt; List of 2 #&gt; $ copy.two.output1:Classes &#39;TaskClassif&#39;, &#39;TaskSupervised&#39;, &#39;Task&#39;, &#39;R6&#39; &lt;TaskClassif:iris&gt; #&gt; $ copy.two.output2:Classes &#39;TaskClassif&#39;, &#39;TaskSupervised&#39;, &#39;Task&#39;, &#39;R6&#39; &lt;TaskClassif:iris&gt; 5.6.2 Special Case: Preprocessing Many PipeOps perform an operation on exactly one Task, and return exactly one Task. They may even not care about the “Target” / “Outcome” variable of that task, and only do some modification of some input data. However, it is usually important to them that the Task on which they perform prediction has the same data columns as the Task on which they train. For these cases, the auxiliary base class PipeOpTaskPreproc exists. It inherits from PipeOp itself, and other PipeOps should use it if they fall in the kind of use-case named above. When inheriting from PipeOpTaskPreproc, one must either implement the train_task and predict_task functions, or the train_dt, predict_dt functions, depending on whether wants to operate on a Task object or on data.tables. In the second case, one can optionally also overload the select_cols function, which chooses which of the incoming Task’s features are given to the train_dt / predict_dt functions. The following will show two examples: PipeOpDropNA, which removes a Task’s rows with missing values during training (and implements train_task and predict_task), and PipeOpScale, which scales a Task’s numeric columns (and implements train_dt, predict_dt, and select_cols). 5.6.2.1 Example: PipeOpDropNA Dropping rows with missing values may be important when training a model that can not handle them. Because mlr3 Tasks only contain a view to the underlying data, it is not necessary to modify data to remove rows with missing values. Instead, the rows can be removed using the Task’s $filter method, which modifies the Task in-place. This is done in the train_task function. We take care that we also set the $state slot to signal that the PipeOp was trained. The predict_task function does not need to do anything; removing missing values during prediction is not as useful, since learners that cannot handle them will just ignore the respective rows. Furthermore, mlr3 expects a Learner to always return just as many predictions as it was given input rows, so a PipeOp that removes Task rows during training can not be used inside a GraphLearner. When we inherit from PipeOpTaskPreproc, it sets the input and output data.tables for us to only accept a single Task. The only thing we do during initialize() is therefore to set an id (which can optionally be changed by the user). The complete PipeOpDropNA can therefore be written as follows. Note that it inherits from PipeOpTaskPreproc, unlike the PipeOpCopyTwo example from above: PipeOpDropNA = R6::R6Class(&quot;PipeOpDropNA&quot;, inherit = PipeOpTaskPreproc, public = list( initialize = function(id = &quot;drop.na&quot;) { super$initialize(id) }, train_task = function(task) { self$state = list() featuredata = task$data(cols = task$feature_names) exclude = apply(is.na(featuredata), 1, any) task$filter(task$row_ids[!exclude]) }, predict_task = function(task) { # nothing to be done task } ) ) To test this PipeOp, we create a small task with missing values: smalliris = iris[(1:5) * 30, ] smalliris[1, 1] = NA smalliris[2, 2] = NA sitask = TaskClassif$new(&quot;smalliris&quot;, as_data_backend(smalliris), &quot;Species&quot;) print(sitask$data()) #&gt; Species Petal.Length Petal.Width Sepal.Length Sepal.Width #&gt; 1: setosa 1.6 0.2 NA 3.2 #&gt; 2: versicolor 3.9 1.4 5.2 NA #&gt; 3: versicolor 4.0 1.3 5.5 2.5 #&gt; 4: virginica 5.0 1.5 6.0 2.2 #&gt; 5: virginica 5.1 1.8 5.9 3.0 We test this by feeding it to a new Graph that uses PipeOpDropNA. gr = Graph$new() gr$add_pipeop(PipeOpDropNA$new()) filtered_task = gr$train(sitask)[[1]] print(filtered_task$data()) #&gt; Species Petal.Length Petal.Width Sepal.Length Sepal.Width #&gt; 1: versicolor 4.0 1.3 5.5 2.5 #&gt; 2: virginica 5.0 1.5 6.0 2.2 #&gt; 3: virginica 5.1 1.8 5.9 3.0 5.6.2.2 Example: PipeOpScaleAlways An often-applied preprocessing step is to simply center and/or scale the data to mean \\(0\\) and standard deviation \\(1\\). This fits the PipeOpTaskPreproc pattern quite well. Because it always replaces all columns that it operates on, and does not require any information about the task’s target, it only needs to overload the train_dt and predict_dt functions. This saves some boilerplate-code from getting the correct feature columns out of the task, and replacing them after modification. Because scaling only makes sense on numeric features, we want to instruct PipeOpTaskPreproc to give us only these numeric columns. We do this by overloading the select_cols function: It is called by the class to determine which columns to give to train_dt and predict_dt. Its input is the Task that is being transformed, and it should return a character vector of all features to work with. When it is not overloaded, it uses all columns; instead, we will set it to only give us numeric columns. Because the levels() of the data table given to train_dt and predict_dt may be different from the levels task’s levels, these functions must also take a levels argument that is a named list of column names indicating their levels. When working with numeric data, this argument can be ignored, but it should be used instead of levels(dt[[column]]) for factorial or character columns. This is the first PipeOp where we will be using the $state slot for something useful: We save the centering offset and scaling coefficient and use it in $predict()! For simplicity, we are not using hyperparameters and will always scale and center all data. Compare this PipeOpScaleAlways operator to the one defined inside the mlr3pipelines package, PipeOpScale, defined in PipeOpScale.R. PipeOpScaleAlways = R6::R6Class(&quot;PipeOpScaleAlways&quot;, inherit = PipeOpTaskPreproc, public = list( initialize = function(id = &quot;scale.always&quot;) { super$initialize(id = id) }, select_cols = function(task) { task$feature_types[type == &quot;numeric&quot;, id] }, train_dt = function(dt, levels) { sc = scale(as.matrix(dt)) self$state = list( center = attr(sc, &quot;scaled:center&quot;), scale = attr(sc, &quot;scaled:scale&quot;) ) sc }, predict_dt = function(dt, levels) { t((t(dt) - self$state$center) / self$state$scale) } ) ) (Note for the observant: If you check PipeOpScale.R from the mlr3pipelines package, you will notice that is uses “get(\"type\")” and “get(\"id\")” instead of “type” and “id”, because the static code checker on CRAN would otherwise complain about references to undefined variables. This is a “problem” with data.table and not exclusive to mlr3pipelines.) We can, again, create a new Graph that uses this PipeOp to test it. Compare the resulting data to the original “iris” Task data printed at the beginning: gr = Graph$new() gr$add_pipeop(PipeOpScaleAlways$new()) result = gr$train(task) result[[1]]$data() #&gt; Species Petal.Length Petal.Width Sepal.Length Sepal.Width #&gt; 1: setosa -1.3358 -1.3111 -0.89767 1.01560 #&gt; 2: setosa -1.3358 -1.3111 -1.13920 -0.13154 #&gt; 3: setosa -1.3924 -1.3111 -1.38073 0.32732 #&gt; 4: setosa -1.2791 -1.3111 -1.50149 0.09789 #&gt; 5: setosa -1.3358 -1.3111 -1.01844 1.24503 #&gt; --- #&gt; 146: virginica 0.8169 1.4440 1.03454 -0.13154 #&gt; 147: virginica 0.7036 0.9192 0.55149 -1.27868 #&gt; 148: virginica 0.8169 1.0504 0.79301 -0.13154 #&gt; 149: virginica 0.9302 1.4440 0.43072 0.78617 #&gt; 150: virginica 0.7602 0.7880 0.06843 -0.13154 5.6.3 Special Case: Preprocessing with Simple Train It is possible to make even further simplifications for many PipeOps that perform mostly the same operation during training and prediction. The point of Task preprocessing is often to modify the training data in mostly the same way as prediction data (but in a way that may depend on training data). Consider constant feature removal, for example: The goal is to remove features that have no variance, or only a single factor level. However, what features get removed must be decided during training, and may only depend on training data. Furthermore, the actual process of removing features is the same during training and prediction. A simplification to make is therefore to have a function get_state(task) which sets the $state slot during training, and a transform(task) function, which gets called both during training and prediction. This is done in the PipeOpTaskPreprocSimple class. Just like PipeOpTaskPreproc, one can inherit from this and overload these functions to get a PipeOp that performs preprocessing with very little boilerplate code. Just like PipeOpTaskPreproc, PipeOpTaskPreprocSimple offers the possibility to instead overload the get_state_dt(dt, levels) and transform_dt(dt, levels) functions (and optionally, again, the select_cols(task) function) to operate on data.table feature data instead of the whole Task. Even some methods that do not use PipeOpTaskPreprocSimple could work in a similar way: The PipeOpScaleAlways example from above will be shown to also work with this paradigm. 5.6.3.1 Example: PipeOpDropConst A typical example of a preprocessing operation that does almost the same operation during training and prediction is an operation that drops features depending on a criterion that is evaluated during training. One simple example of this is dropping constant features. Because the mlr3 Task class offers a flexible view on underlying data, it is most efficient to drop columns from the task directly using its $select() function, so the get_state_dt(dt, levels) / transform_dt(dt, levels) functions will not get used; instead we overload the get_state(task) and transform(task) functions. The get_state() function’s result is saved to the $state slot, so we want to return something that is useful for dropping features. We choose to save the names of all the columns that have nonzero variance. For brevity, we use length(unique(column)) &gt; 1 to check whether more than one distinct value is present; a more sophisticated version could have a tolerance parameter for numeric values that are very close to each other. The transform() function is evaluated both during training and prediction, and can rely on the $state slot being present. All it does here is call the Task$select function with the columns we chose to keep. The full PipeOp could be written as follows: PipeOpDropConst = R6::R6Class(&quot;PipeOpDropConst&quot;, inherit = PipeOpTaskPreprocSimple, public = list( initialize = function(id = &quot;drop.const&quot;) { super$initialize(id = id) }, get_state = function(task) { data = task$data(cols = task$feature_names) nonconst = sapply(data, function(column) length(unique(column)) &gt; 1) list(cnames = colnames(data)[nonconst]) }, transform = function(task) { task$select(self$state$cnames) } ) ) This can be tested using the first five rows of the “Iris” Task, for which one feature (\"Petal.Width\") is constant: irishead = task$clone()$filter(1:5) irishead$data() #&gt; Species Petal.Length Petal.Width Sepal.Length Sepal.Width #&gt; 1: setosa 1.4 0.2 5.1 3.5 #&gt; 2: setosa 1.4 0.2 4.9 3.0 #&gt; 3: setosa 1.3 0.2 4.7 3.2 #&gt; 4: setosa 1.5 0.2 4.6 3.1 #&gt; 5: setosa 1.4 0.2 5.0 3.6 gr = Graph$new()$add_pipeop(PipeOpDropConst$new()) dropped_task = gr$train(irishead)[[1]] dropped_task$data() #&gt; Species Petal.Length Sepal.Length Sepal.Width #&gt; 1: setosa 1.4 5.1 3.5 #&gt; 2: setosa 1.4 4.9 3.0 #&gt; 3: setosa 1.3 4.7 3.2 #&gt; 4: setosa 1.5 4.6 3.1 #&gt; 5: setosa 1.4 5.0 3.6 We can also see that the $state was correctly set. Calling $predict() with this graph, even with different data (the whole Iris Task!) will still drop the \"Petal.Width\" column, as it should. gr$pipeops$drop.const$state #&gt; $cnames #&gt; [1] &quot;Petal.Length&quot; &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; #&gt; #&gt; $affected_cols #&gt; [1] &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; #&gt; #&gt; $intasklayout #&gt; id type #&gt; 1: Petal.Length numeric #&gt; 2: Petal.Width numeric #&gt; 3: Sepal.Length numeric #&gt; 4: Sepal.Width numeric #&gt; #&gt; $outtasklayout #&gt; id type #&gt; 1: Petal.Length numeric #&gt; 2: Sepal.Length numeric #&gt; 3: Sepal.Width numeric dropped_predict = gr$predict(task)[[1]] dropped_predict$data() #&gt; Species Petal.Length Sepal.Length Sepal.Width #&gt; 1: setosa 1.4 5.1 3.5 #&gt; 2: setosa 1.4 4.9 3.0 #&gt; 3: setosa 1.3 4.7 3.2 #&gt; 4: setosa 1.5 4.6 3.1 #&gt; 5: setosa 1.4 5.0 3.6 #&gt; --- #&gt; 146: virginica 5.2 6.7 3.0 #&gt; 147: virginica 5.0 6.3 2.5 #&gt; 148: virginica 5.2 6.5 3.0 #&gt; 149: virginica 5.4 6.2 3.4 #&gt; 150: virginica 5.1 5.9 3.0 5.6.3.2 Example: PipeOpScaleAlwaysSimple This example will show how a PipeOpTaskPreprocSimple can be used when only working on feature data in form of a data.table. Instead of calling the scale() function, the center and scale values are calculated directly and saved to the $state slot. The transform_dt function will then perform the same operation during both training and prediction: subtract the center and divide by the scale value. As in the PipeOpScaleAlways example above, we use select_cols() so that we only work on numeric columns. PipeOpScaleAlwaysSimple = R6::R6Class(&quot;PipeOpScaleAlwaysSimple&quot;, inherit = PipeOpTaskPreprocSimple, public = list( initialize = function(id = &quot;scale.always.simple&quot;) { super$initialize(id = id) }, select_cols = function(task) { task$feature_types[type == &quot;numeric&quot;, id] }, get_state_dt = function(dt, levels) { list( center = sapply(dt, mean), scale = sapply(dt, sd) ) }, transform_dt = function(dt, levels) { t((t(dt) - self$state$center) / self$state$scale) } ) ) We can compare this PipeOp to the one above to show that it behaves the same. gr = Graph$new()$add_pipeop(PipeOpScaleAlways$new()) result_posa = gr$train(task)[[1]] gr = Graph$new()$add_pipeop(PipeOpScaleAlwaysSimple$new()) result_posa_simple = gr$train(task)[[1]] result_posa$data() #&gt; Species Petal.Length Petal.Width Sepal.Length Sepal.Width #&gt; 1: setosa -1.3358 -1.3111 -0.89767 1.01560 #&gt; 2: setosa -1.3358 -1.3111 -1.13920 -0.13154 #&gt; 3: setosa -1.3924 -1.3111 -1.38073 0.32732 #&gt; 4: setosa -1.2791 -1.3111 -1.50149 0.09789 #&gt; 5: setosa -1.3358 -1.3111 -1.01844 1.24503 #&gt; --- #&gt; 146: virginica 0.8169 1.4440 1.03454 -0.13154 #&gt; 147: virginica 0.7036 0.9192 0.55149 -1.27868 #&gt; 148: virginica 0.8169 1.0504 0.79301 -0.13154 #&gt; 149: virginica 0.9302 1.4440 0.43072 0.78617 #&gt; 150: virginica 0.7602 0.7880 0.06843 -0.13154 result_posa_simple$data() #&gt; Species Petal.Length Petal.Width Sepal.Length Sepal.Width #&gt; 1: setosa -1.3358 -1.3111 -0.89767 1.01560 #&gt; 2: setosa -1.3358 -1.3111 -1.13920 -0.13154 #&gt; 3: setosa -1.3924 -1.3111 -1.38073 0.32732 #&gt; 4: setosa -1.2791 -1.3111 -1.50149 0.09789 #&gt; 5: setosa -1.3358 -1.3111 -1.01844 1.24503 #&gt; --- #&gt; 146: virginica 0.8169 1.4440 1.03454 -0.13154 #&gt; 147: virginica 0.7036 0.9192 0.55149 -1.27868 #&gt; 148: virginica 0.8169 1.0504 0.79301 -0.13154 #&gt; 149: virginica 0.9302 1.4440 0.43072 0.78617 #&gt; 150: virginica 0.7602 0.7880 0.06843 -0.13154 5.6.4 Hyperparameters mlr3pipelines uses the paradox package to define parameter spaces for PipeOps. Parameters for PipeOps can modify their behaviour in certain ways, e.g. switch centering or scaling off in the PipeOpScale operator. The unified interface makes it possible to have parameters for whole Graphs that modify the individual PipeOp’s behaviour. The Graphs, when encapsuled in GraphLearners, can even be tuned using the tuning functionality in mlr3tuning. Hyperparameters are declared during initialization, when calling the PipeOp’s $initialize() function, by giving a param_set argument. The param_set must be a ParamSet from the paradox package; see the mlr3book for more information on how to define parameter spaces. After construction, the ParamSet can be accessed through the $param_set slot. While it is possible to modify this ParamSet, using e.g. the $add() and $add_dep() functions, after adding it to the PipeOp, it is strongly advised against. Hyperparameters can be set and queried through the $values slot. When setting hyperparameters, they are automatically checked to satisfy all conditions set by the $param_set, so it is not necessary to type check them. Be aware that it is always possible to remove hyperparameter values. When a PipeOp is initialized, it usually does not have any parameter values—$values takes the value list(). It is possible to set initial parameter values in the $initialize() constructor; this must be done after the super$initialize() call where the corresponding ParamSet must be supplied. This is because setting $values checks against the current $param_set, which would fail if the $param_set was not set yet. When using an underlying library function (the scale function in PipeOpScale, say), then there is usually a “default” behaviour of that function when a parameter is not given. It is good practice to use this default behaviour whenever a parameter is not set (or when it was removed). This can easily be done when using the mlr3misc library’s invoke() function, which has functionality similar to base::do.call(). 5.6.4.1 Hyperparameter Example: PipeOpScale How to use hyperparameters can best be shown through the example of PipeOpScale, which is very similar to the example above, PipeOpScaleAlways. The difference is made by the presence of hyperparameters. PipeOpScale constructs a ParamSet in its $initialize function and passes this on to the super$initialize function: PipeOpScale$public_methods$initialize #&gt; function (id = &quot;scale&quot;, param_vals = list()) #&gt; { #&gt; ps = ParamSet$new(params = list(ParamLgl$new(&quot;center&quot;, default = TRUE), #&gt; ParamLgl$new(&quot;scale&quot;, default = TRUE))) #&gt; super$initialize(id = id, param_set = ps, param_vals = param_vals) #&gt; } #&gt; &lt;bytecode: 0x152333a0&gt; #&gt; &lt;environment: namespace:mlr3pipelines&gt; The user has access to this and can set and get parameters. Types are automatically checked: pss = PipeOpScale$new() print(pss$param_set) #&gt; ParamSet: scale #&gt; id class lower upper levels default value #&gt; 1: center ParamLgl NA NA TRUE,FALSE TRUE #&gt; 2: scale ParamLgl NA NA TRUE,FALSE TRUE #&gt; 3: affect_columns ParamUty NA NA &lt;NoDefault&gt; pss$values$center = FALSE print(pss$values) #&gt; $center #&gt; [1] FALSE pss$values$scale = &quot;TRUE&quot; # bad input is checked! #&gt; Error in (function (xs) : Assertion on &#39;xs&#39; failed: scale: Must be of type &#39;logical flag&#39;, not &#39;character&#39;. How PipeOpScale handles its parameters can be seen in its $train method: It gets the relevant parameters from its $values slot and uses them in the mlr3misc::invoke call. This has the advantage over calling scale() directly that if a parameter is not given, its default value from the base::scale function will be used. PipeOpScale$public_methods$train #&gt; function (dt, levels) #&gt; { #&gt; sc = invoke(scale, as.matrix(dt), .args = self$param_set$values) #&gt; self$state = list(center = attr(sc, &quot;scaled:center&quot;) %??% #&gt; 0, scale = attr(sc, &quot;scaled:scale&quot;) %??% 1) #&gt; constfeat = self$state$scale == 0 #&gt; self$state$scale[constfeat] = 1 #&gt; sc[, constfeat] = 0 #&gt; sc #&gt; } #&gt; &lt;bytecode: 0x152361d8&gt; #&gt; &lt;environment: namespace:mlr3pipelines&gt; Another change that is necessary compared to PipeOpScaleAlways is that the attributes \"scaled:scale\" and \"scaled:center\" are not always present, depending on parameters, and possibly need to be set to default values \\(1\\) or \\(0\\), respectively. It is now even possible (if a bit pointless) to call PipeOpScale with both scale and center set to FALSE, which returns the original dataset, unchanged. pss$values$scale = FALSE pss$values$center = FALSE gr = Graph$new() gr$add_pipeop(pss) result = gr$train(task) result[[1]]$data() #&gt; Species Petal.Length Petal.Width Sepal.Length Sepal.Width #&gt; 1: setosa 1.4 0.2 5.1 3.5 #&gt; 2: setosa 1.4 0.2 4.9 3.0 #&gt; 3: setosa 1.3 0.2 4.7 3.2 #&gt; 4: setosa 1.5 0.2 4.6 3.1 #&gt; 5: setosa 1.4 0.2 5.0 3.6 #&gt; --- #&gt; 146: virginica 5.2 2.3 6.7 3.0 #&gt; 147: virginica 5.0 1.9 6.3 2.5 #&gt; 148: virginica 5.2 2.0 6.5 3.0 #&gt; 149: virginica 5.4 2.3 6.2 3.4 #&gt; 150: virginica 5.1 1.8 5.9 3.0 "],
["special-tasks.html", "6 Special Tasks ", " 6 Special Tasks "],
["survival.html", "6.1 Survival Analysis", " 6.1 Survival Analysis The package mlr3survival extends mlr3 with the following objects for survival analysis: TaskSurv to define (right-censored) survival tasks LearnerSurv as base class for survival learners PredictionSurv as specialized class for Prediction objects MeasureSurv as specialized class for performance measures In this example we demonstrate the basic functionality of the package on the rats data from the survival package. This task ships as pre-defined TaskSurv with mlr3survival. library(mlr3survival) task = mlr_tasks$get(&quot;rats&quot;) print(task) #&gt; &lt;TaskSurv:rats&gt; (300 x 5) #&gt; Target: time, status #&gt; Properties: - #&gt; Features (3): #&gt; * int (2): litter, rx #&gt; * fct (1): sex mlr3viz::autoplot(task) # the target column is a survival object: head(task$truth()) #&gt; [1] 101+ 49 104+ 91+ 104+ 102+ # kaplan meier estimator of survival times task$survfit() #&gt; Call: survfit(formula = f, data = self$data(cols = vars)) #&gt; #&gt; n events median 0.95LCL 0.95UCL #&gt; 300 42 NA NA NA # kaplan-meier estimator of survival times # stratify on binarized age variable sf = task$survfit(&quot;sex&quot;) print(sf) #&gt; Call: survfit(formula = f, data = self$data(cols = vars)) #&gt; #&gt; n events median 0.95LCL 0.95UCL #&gt; sex=f 150 40 NA NA NA #&gt; sex=m 150 2 NA NA NA # plot survfit object with ggfortify library(&quot;ggfortify&quot;) #&gt; Loading required package: ggplot2 autoplot(sf) Now, we conduct a small benchmark study on the rats task using all the integrated survival learners: # integrated learners learners = mlr_learners$keys(&quot;^surv&quot;) print(learners) #&gt; [1] &quot;surv.coxph&quot; &quot;surv.featureless&quot; &quot;surv.ranger&quot; &quot;surv.rpart&quot; measures = mlr_measures$keys(&quot;^surv&quot;) print(measures) #&gt; [1] &quot;surv.harrells_c&quot; &quot;surv.unos_c&quot; set.seed(1) bmr = benchmark(expand_grid(task, learners, &quot;cv&quot;)) print(bmr) #&gt; &lt;BenchmarkResult&gt; of 40 iterations in 4 resamplings mlr3viz::autoplot(bmr, measure = measures[1]) mlr3viz::autoplot(bmr, measure = measures[2]) "],
["ordinal.html", "6.2 Ordinal Analysis", " 6.2 Ordinal Analysis "],
["spatial.html", "6.3 Spatial Analysis", " 6.3 Spatial Analysis "],
["functional.html", "6.4 Functional Analysis", " 6.4 Functional Analysis "],
["multilabel.html", "6.5 Multilabel Analysis", " 6.5 Multilabel Analysis "],
["cost-sens.html", "6.6 Cost-Sensitive Classification", " 6.6 Cost-Sensitive Classification Imagine you are an analyst for a big credit institution. Let’s also assume that a correct decision of the bank would result in 35% of the profit at the end of a specific period. A correct decision means that the bank predicts that a customer will pay their bills (hence would obtain a loan), and the customer indeed has good credit. On the other hand, a wrong decision means that the bank predicts that the customer’s credit is in good standing, but the opposite is true. This would result in a loss of 100% of the given loan. Good Customer (truth) Bad Customer (truth) Good Customer (predicted) + 0.35 - 1.0 Bad Customer (predicted) 0 0 Expressed as costs (instead of profit), we can write down the cost-matrix as follows: costs = matrix(c(-0.35, 0, 1, 0), nrow = 2) dimnames(costs) = list(response = c(&quot;good&quot;, &quot;bad&quot;), truth = c(&quot;good&quot;, &quot;bad&quot;)) print(costs) #&gt; truth #&gt; response good bad #&gt; good -0.35 1 #&gt; bad 0.00 0 An exemplary data set for such a problem is the German Credit task: library(mlr3) task = mlr_tasks$get(&quot;german_credit&quot;) table(task$truth()) #&gt; #&gt; good bad #&gt; 700 300 The data has 70% customers who are able to pay back their credit, and 30% bad customers who default on the debt. A manager, who doesn’t have any model, could decide to give either everybody a credit or to give nobody a credit. The resulting costs for the German credit data are: # nobody: (700 * costs[2, 1] + 300 * costs[2, 2]) / 1000 #&gt; [1] 0 # everybody (700 * costs[1, 1] + 300 * costs[1, 2]) / 1000 #&gt; [1] 0.055 If the average loan is $20,000, the credit institute would lose more than one million dollar if it would grant everybody a credit: # average profit * average loan * number of customers 0.055 * 20000 * 1000 #&gt; [1] 1100000 Our goal is to find a model which minimizes the costs (and thereby maximizes the expected profit). 6.6.1 A First Model For our first model, we choose an ordinary logistic regression (implemented in the add-on package mlr3learners). We first create a classification task, then resample the model using a 10-fold cross validation and extract the resulting confusion matrix: library(mlr3learners) learner = mlr_learners$get(&quot;classif.log_reg&quot;) rr = resample(task, learner, &quot;cv&quot;) confusion = rr$prediction$confusion print(confusion) #&gt; truth #&gt; response good bad #&gt; good 607 156 #&gt; bad 93 144 To calculate the average costs like above, we can simply multiply the elements of the confusion matrix with the elements of the previously introduced cost matrix, and sum the values of the resulting matrix: avg_costs = sum(confusion * costs) / 1000 print(avg_costs) #&gt; [1] -0.05645 With an average loan of $20,000, the logistic regression yields the following costs: avg_costs * 20000 * 1000 #&gt; [1] -1129000 Instead of losing over $1,000,000, the credit institute now can expect a profit of more than $1,000,000. 6.6.2 Cost-sensitive Measure Our natural next step would be to further improve the modeling step in order to maximize the profit. For this purpose we first create a cost-sensitive classification measure which calculates the costs based on our cost matrix. This allows us to conveniently quantify and compare modeling decisions. Fortunately, there already is a predefined measure Measure for this purpose: MeasureClassifCosts: cost_measure = MeasureClassifCosts$new(&quot;credit_costs&quot;, costs) print(cost_measure) #&gt; &lt;MeasureClassifCosts:credit_costs&gt; #&gt; Packages: - #&gt; Range: [-Inf, Inf] #&gt; Minimize: TRUE #&gt; Properties: requires_task #&gt; Predict type: response If we now call resample() or benchmark(), the cost-sensitive measures will be evaluated. We compare the logistic regression to a simple featureless learner and to a random forest from package ranger : learners = mlr_learners$mget(c(&quot;classif.log_reg&quot;, &quot;classif.featureless&quot;, &quot;classif.ranger&quot;)) bmr = benchmark(expand_grid(task, learners, &quot;cv&quot;)) bmr$aggregate(cost_measure) #&gt; hash resample_result task_id learner_id #&gt; 1: d2889368a18c076d &lt;ResampleResult&gt; german_credit classif.log_reg #&gt; 2: 7eb031e3741ce663 &lt;ResampleResult&gt; german_credit classif.featureless #&gt; 3: 06ffe67495186afa &lt;ResampleResult&gt; german_credit classif.ranger #&gt; resampling_id credit_costs #&gt; 1: cv -0.05665 #&gt; 2: cv 0.05500 #&gt; 3: cv -0.04865 As expected, the featureless learner is performing comparably bad. The logistic regression and the random forest work equally well. 6.6.3 Thresholding Although we now correctly evaluate the models in a cost-sensitive fashion, the models themselves are unaware of the classification costs. They assume the same costs for both wrong classification decisions (false positives and false negatives). Some learners natively support cost-sensitive classification (e.g., XXX). However, we will concentrate on a more generic approach which works for all models which can predict probabilities for class labels: thresholding. Most learners can calculate the probability \\(p\\) for the positive class. If \\(p\\) exceeds the threshold \\(0.5\\), they predict the positive class, and the negative class otherwise. For our binary classification case of the credit data, the we primarily want to minimize the errors where the model predicts “good”, but truth is “bad” (i.e., the number of false positives) as this is the more expensive error. If we now increase the threshold to values \\(&gt; 0.5\\), we reduce the number of false negatives. Note that we increase the number of false positives simultaneously, or, in other words, we are trading false positives for false negatives. # fit models with probability prediction learner = mlr_learners$get(&quot;classif.log_reg&quot;, predict_type = &quot;prob&quot;) rr = resample(task, learner, &quot;cv&quot;) p = rr$prediction print(p) #&gt; &lt;PredictionClassif&gt; for 1000 observations: #&gt; row_id truth response prob.good prob.bad #&gt; 1: 7 good good 0.9364 0.06356 #&gt; 2: 12 bad bad 0.1563 0.84369 #&gt; 3: 43 good good 0.6406 0.35944 #&gt; --- #&gt; 998: 963 good good 0.8204 0.17963 #&gt; 999: 976 good good 0.9167 0.08334 #&gt; 1000: 993 good good 0.8080 0.19201 # helper function to try different threshold values interactively with_threshold = function(p, th) { p$set_threshold(th) list(confusion = p$confusion, costs = p$score(measures = cost_measure, task = task)) } with_threshold(p, 0.5) #&gt; $confusion #&gt; truth #&gt; response good bad #&gt; good 603 159 #&gt; bad 97 141 #&gt; #&gt; $costs #&gt; credit_costs #&gt; -0.05205 with_threshold(p, 0.75) #&gt; $confusion #&gt; truth #&gt; response good bad #&gt; good 471 72 #&gt; bad 229 228 #&gt; #&gt; $costs #&gt; credit_costs #&gt; -0.09285 with_threshold(p, 1.0) #&gt; $confusion #&gt; truth #&gt; response good bad #&gt; good 1 1 #&gt; bad 699 299 #&gt; #&gt; $costs #&gt; credit_costs #&gt; 0.00065 # TODO: include plot of threshold vs performance Instead of manually trying different threshold values, we here use optimize() to find a good threshold value w.r.t. our performance measure: # simple wrapper function which takes a threshold and returns the resulting model performance # this wrapper is passed to optimize() to find its minimum for thresholds in [0.5, 1] f = function(th) { with_threshold(p, th)$costs } best = optimize(f, c(0.5, 1)) print(best) #&gt; $minimum #&gt; [1] 0.7661 #&gt; #&gt; $objective #&gt; credit_costs #&gt; -0.0956 # optimized confusion matrix: with_threshold(p, best$minimum)$confusion #&gt; truth #&gt; response good bad #&gt; good 456 64 #&gt; bad 244 236 The function optimize() is intended for unimodal functions and therefore may converge to a local optimum here. See below for better alternatives to find good threshold values. 6.6.4 Threshold Tuning To be continued… threshold tuning as pipeline operator joint hyperparameter optimization "],
["model-interpr.html", "7 Model Interpretation with mlr3 ", " 7 Model Interpretation with mlr3 "],
["iml.html", "7.1 IML", " 7.1 IML "],
["dalex.html", "7.2 Dalex", " 7.2 Dalex "],
["use-cases.html", "8 Use Cases", " 8 Use Cases This chapter is a collection of use cases to showcase mlr3. "],
["appendix.html", "9 Appendix ", " 9 Appendix "],
["list-learners.html", "9.1 Integrated Learners", " 9.1 Integrated Learners Id Feature Types Required package Properties Predict Types classif.debug lgl, int, dbl, chr, fct, ord Missings, Multiclass, Twoclass response, prob classif.featureless lgl, int, dbl, chr, fct, ord Importance, Missings, Multiclass, Selected Features, Twoclass response, prob classif.glmnet int, dbl glmnet Multiclass, Twoclass, Weights response, prob classif.kknn lgl, int, dbl, fct, ord kknn, withr Multiclass, Twoclass response, prob classif.lda lgl, int, dbl, fct, ord MASS Multiclass, Twoclass, Weights response, prob classif.log_reg lgl, int, dbl, chr, fct, ord stats Twoclass, Weights response, prob classif.naive_bayes lgl, int, dbl, fct e1071 Multiclass, Twoclass response, prob classif.qda lgl, int, dbl, fct, ord MASS Multiclass, Twoclass, Weights response, prob classif.ranger lgl, int, dbl, chr, fct, ord ranger Importance, Multiclass, Oob Error, Twoclass, Weights response, prob classif.rpart lgl, int, dbl, chr, fct, ord rpart Importance, Missings, Multiclass, Selected Features, Twoclass, Weights response, prob classif.svm int, dbl e1071 Multiclass, Twoclass response, prob classif.xgboost int, dbl xgboost Importance, Missings, Multiclass, Twoclass, Weights response, prob regr.featureless lgl, int, dbl, chr, fct, ord stats Importance, Missings, Selected Features response, se regr.glmnet int, dbl glmnet Weights response regr.kknn lgl, int, dbl, fct, ord kknn, withr response regr.km int, dbl DiceKriging response, se regr.lm int, dbl, fct stats Weights response, se regr.ranger lgl, int, dbl, chr, fct, ord ranger Importance, Oob Error, Weights response, se regr.rpart lgl, int, dbl, chr, fct, ord rpart Importance, Missings, Selected Features, Weights response regr.svm int, dbl e1071 response regr.xgboost int, dbl xgboost Importance, Missings, Weights response surv.coxph lgl, int, dbl, fct survival Weights risk surv.featureless lgl, int, dbl, chr, fct, ord Importance, Missings, Selected Features risk surv.ranger lgl, int, dbl, chr, fct, ord ranger Importance, Oob Error, Weights risk surv.rpart lgl, int, dbl, chr, fct, ord rpart Importance, Missings, Selected Features, Weights risk "],
["list-filters.html", "9.2 Integrated Filter Methods", " 9.2 Integrated Filter Methods 9.2.1 Standalone filter methods Name Task task_properties param_set Features Package cmim Classif &amp; Regr character(0) Integer, Numeric, Factor, Ordered praznik gain_ratio Classif &amp; Regr character(0) Integer, Numeric, Factor, Ordered FSelectorRcpp information_gain Classif &amp; Regr character(0) Integer, Numeric, Factor, Ordered FSelectorRcpp symmetrical_uncertainty Classif &amp; Regr character(0) Integer, Numeric, Factor, Ordered FSelectorRcpp variance Classif &amp; Regr character(0) Integer, Numeric stats auc Classif twoclass Integer, Numeric Metrics disr Classif character(0) Integer, Numeric, Factor, Ordered praznik embedded Classif character(0) c(“logical”, “integer”, “numeric”, “character”, “factor”, “ordered”) rpart jmi Classif character(0) Integer, Numeric, Factor, Ordered praznik kruskal_test Classif character(0) Integer, Numeric stats mim Classif character(0) Integer, Numeric, Factor, Ordered praznik njmim Classif character(0) Integer, Numeric, Factor, Ordered praznik carscore Regr character(0) numeric care correlation Regr character(0) Integer, Numeric stats 9.2.2 Algorithms With Embedded Filter Methods Please take a look at the implementation details of the respective learner which feature types are supported by the filter. #&gt; [1] &quot;classif.featureless&quot; &quot;classif.ranger&quot; &quot;classif.rpart&quot; #&gt; [4] &quot;classif.xgboost&quot; &quot;regr.featureless&quot; &quot;regr.ranger&quot; #&gt; [7] &quot;regr.rpart&quot; &quot;regr.xgboost&quot; &quot;surv.featureless&quot; #&gt; [10] &quot;surv.ranger&quot; &quot;surv.rpart&quot; "],
["references.html", "References", " References "]
]
